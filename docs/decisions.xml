<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>V-Sekai - Manuals</title>
<link>https://v-sekai.github.io/manuals/decisions.html</link>
<atom:link href="https://v-sekai.github.io/manuals/decisions.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.3.450</generator>
<lastBuildDate>Mon, 23 Oct 2023 22:52:52 GMT</lastBuildDate>
<item>
  <title></title>
  <link>https://v-sekai.github.io/manuals/decisions/present-proposal-template.html</link>
  <description><![CDATA[ 



<section id="overcoming-a-v-sekai-limit" class="level1">
<h1>Overcoming a V-Sekai limit</h1>
<section id="metadata" class="level2">
<h2 class="anchored" data-anchor-id="metadata">Metadata</h2>
<ul>
<li>Status: Proposed <!-- Draft | Proposed | Rejected | Accepted | Deprecated | Superseded by --><br>
</li>
<li>Deciders: V-Sekai<br>
</li>
<li>Tags: V-Sekai</li>
</ul>
</section>
<section id="the-backdrop" class="level2">
<h2 class="anchored" data-anchor-id="the-backdrop">The Backdrop</h2>
</section>
<section id="the-challenge" class="level2">
<h2 class="anchored" data-anchor-id="the-challenge">The Challenge</h2>
</section>
<section id="the-strategy" class="level2">
<h2 class="anchored" data-anchor-id="the-strategy">The Strategy</h2>
</section>
<section id="the-upside" class="level2">
<h2 class="anchored" data-anchor-id="the-upside">The Upside</h2>
</section>
<section id="the-downside" class="level2">
<h2 class="anchored" data-anchor-id="the-downside">The Downside</h2>
</section>
<section id="the-road-not-taken" class="level2">
<h2 class="anchored" data-anchor-id="the-road-not-taken">The Road Not Taken</h2>
</section>
<section id="the-infrequent-use-case" class="level2">
<h2 class="anchored" data-anchor-id="the-infrequent-use-case">The Infrequent Use Case</h2>
</section>
<section id="in-core-and-done-by-us" class="level2">
<h2 class="anchored" data-anchor-id="in-core-and-done-by-us">In Core and Done by Us?</h2>
</section>
<section id="further-reading" class="level2">
<h2 class="anchored" data-anchor-id="further-reading">Further Reading</h2>
<ul>
<li><a href="https://github.com/v-sekai/">V-Sekai</a></li>
<li>AI assists this article.</li>
</ul>


</section>
</section>

 ]]></description>
  <guid>https://v-sekai.github.io/manuals/decisions/present-proposal-template.html</guid>
  <pubDate>Mon, 23 Oct 2023 22:52:52 GMT</pubDate>
</item>
<item>
  <title></title>
  <link>https://v-sekai.github.io/manuals/decisions/20231022-hover-to-ocean-game.html</link>
  <description><![CDATA[ 



<section id="navigating-new-horizons-a-proposal-for-expanding-exploration-in-v-sekai" class="level1">
<h1>Navigating New Horizons: A Proposal for Expanding Exploration in V-Sekai</h1>
<section id="metadata" class="level2">
<h2 class="anchored" data-anchor-id="metadata">Metadata</h2>
<ul>
<li>Status: Proposed</li>
<li>Deciders: V-Sekai, fire, cyberpunkmermaid</li>
<li>Tags: V-Sekai</li>
</ul>
</section>
<section id="the-backdrop" class="level2">
<h2 class="anchored" data-anchor-id="the-backdrop">The Backdrop</h2>
<p>The player starts with a hovercar and uses an upgrade to transform it into a hover submarine for ocean exploration. This involves navigating through surreal landscapes, encountering storms, avoiding volcanic eruptions, exploring coral reefs, and finally reaching a new island.</p>
</section>
<section id="the-challenge" class="level2">
<h2 class="anchored" data-anchor-id="the-challenge">The Challenge</h2>
<p>The main challenge lies in transitioning from land to underwater exploration while maintaining an engaging experience for the player. Creating diverse and interactive environments such as the cyber surrealist/impressionist voxel blocks landscape and the vibrant coral reef poses its own set of challenges.</p>
</section>
<section id="the-strategy" class="level2">
<h2 class="anchored" data-anchor-id="the-strategy">The Strategy</h2>
<p>We can use voxel-based rendering for creating the surreal landscapes and procedural generation for creating diverse marine life in the coral reef. For the transition from hovercar to hover submarine, we can design a special animation sequence that showcases the transformation.</p>
</section>
<section id="the-upside" class="level2">
<h2 class="anchored" data-anchor-id="the-upside">The Upside</h2>
<p>Implementing this would result in a unique and engaging gaming experience that combines elements of exploration, adventure, and survival. It would also showcase the capabilities of V-Sekai in creating diverse and immersive virtual worlds.</p>
</section>
<section id="the-downside" class="level2">
<h2 class="anchored" data-anchor-id="the-downside">The Downside</h2>
<p>The complexity involved in implementing the proposed features would require significant resources and expertise. There’s also a risk that the final product may not meet the expectations due to technical limitations or unforeseen challenges.</p>
</section>
<section id="the-road-not-taken" class="level2">
<h2 class="anchored" data-anchor-id="the-road-not-taken">The Road Not Taken</h2>
<p>An alternative approach could be to simplify the journey by limiting the number of environments or removing the transformation aspect. However, this would likely result in a less engaging experience for the player.</p>
</section>
<section id="in-core-and-done-by-us" class="level2">
<h2 class="anchored" data-anchor-id="in-core-and-done-by-us">In Core and Done by Us?</h2>
<p>Yes, this proposal aligns with the core values of V-Sekai and can be implemented by our team with the right resources and planning.</p>
</section>
<section id="further-reading" class="level2">
<h2 class="anchored" data-anchor-id="further-reading">Further Reading</h2>
<ul>
<li><a href="https://github.com/v-sekai/">V-Sekai</a></li>
</ul>


</section>
</section>

 ]]></description>
  <guid>https://v-sekai.github.io/manuals/decisions/20231022-hover-to-ocean-game.html</guid>
  <pubDate>Mon, 23 Oct 2023 22:52:52 GMT</pubDate>
</item>
<item>
  <title></title>
  <link>https://v-sekai.github.io/manuals/decisions/20231020-llama-cpp-mistral-npc.html</link>
  <description><![CDATA[ 



<section id="llama-cpp-ai-model-npc" class="level1">
<h1>Llama CPP AI Model NPC</h1>
<section id="metadata" class="level2">
<h2 class="anchored" data-anchor-id="metadata">Metadata</h2>
<ul>
<li>Status: Proposed</li>
<li>Deciders: V-Sekai<br>
</li>
<li>Tags: V-Sekai, AI, NPC, Godot 4</li>
</ul>
</section>
<section id="the-backdrop" class="level2">
<h2 class="anchored" data-anchor-id="the-backdrop">The Backdrop</h2>
<p>In the realm of game development, creating engaging and dynamic Non-Player Characters (NPCs) is a challenging task. With advancements in AI, there’s an opportunity to leverage these technologies to enhance NPC interactions.</p>
</section>
<section id="the-challenge" class="level2">
<h2 class="anchored" data-anchor-id="the-challenge">The Challenge</h2>
<p>The challenge lies in effectively using various AI models to create the “brain” of an AI NPC in Godot 4. This involves generating realistic and contextually appropriate dialogues for the NPCs.</p>
</section>
<section id="the-strategy" class="level2">
<h2 class="anchored" data-anchor-id="the-strategy">The Strategy</h2>
<p>The strategy involves using a local GPT4All model for generating AI-based NPC dialogue. The script includes several exported variables that can be set in the Godot editor, such as <code>npc_background_directions</code>, <code>sample_npc_question_prompt</code>, and <code>sample_npc_prompt_response</code>. These variables are used to configure the behavior of the AI dialogue generation.</p>
<p>Key functions include:</p>
<ul>
<li><code>call_model(prompt)</code>: Calls the local model with a given prompt.</li>
<li><code>set_model(new_model_name : String)</code>: Sets the model name.</li>
</ul>
</section>
<section id="the-upside" class="level2">
<h2 class="anchored" data-anchor-id="the-upside">The Upside</h2>
<p>Leveraging AI for NPC dialogue generation can lead to more dynamic and engaging gameplay. It allows for a wide range of responses and can adapt to different player inputs.</p>
</section>
<section id="the-downside" class="level2">
<h2 class="anchored" data-anchor-id="the-downside">The Downside</h2>
<p>AI models require significant computational resources and may not be suitable for all platforms. Additionally, ensuring the appropriateness and quality of generated dialogues can be challenging.</p>
</section>
<section id="the-road-not-taken" class="level2">
<h2 class="anchored" data-anchor-id="the-road-not-taken">The Road Not Taken</h2>
<p>Traditional methods of NPC dialogue generation involve pre-scripted dialogues or simple decision trees. While these methods are less resource-intensive, they lack the dynamism and adaptability of AI-generated dialogues.</p>
</section>
<section id="the-infrequent-use-case" class="level2">
<h2 class="anchored" data-anchor-id="the-infrequent-use-case">The Infrequent Use Case</h2>
<p>In games with minimal NPC interactions or where dialogue is not a significant aspect of gameplay, the use of AI for dialogue generation may not be necessary.</p>
</section>
<section id="in-core-and-done-by-us" class="level2">
<h2 class="anchored" data-anchor-id="in-core-and-done-by-us">In Core and Done by Us?</h2>
<p>Yes, the implementation of this strategy is done by us in the core of our game development process.</p>
</section>
<section id="further-reading" class="level2">
<h2 class="anchored" data-anchor-id="further-reading">Further Reading</h2>
<ul>
<li><a href="https://github.com/v-sekai/">V-Sekai GitHub</a></li>
<li>AI assists this article.<br>
</li>
<li><a href="https://github.com/OpenAccess-AI-Collective/axolotl/tree/main/examples/mistral">Axolotl Mistral Example</a></li>
</ul>


</section>
</section>

 ]]></description>
  <guid>https://v-sekai.github.io/manuals/decisions/20231020-llama-cpp-mistral-npc.html</guid>
  <pubDate>Mon, 23 Oct 2023 22:52:52 GMT</pubDate>
</item>
<item>
  <title></title>
  <link>https://v-sekai.github.io/manuals/decisions/20231019-quest3-mocap-steam-lighthouse.html</link>
  <description><![CDATA[ 



<section id="valve-vr-ecosystem-with-quest-3" class="level1">
<h1>Valve VR Ecosystem with Quest 3</h1>
<section id="metadata" class="level2">
<h2 class="anchored" data-anchor-id="metadata">Metadata</h2>
<ul>
<li>Status: Proposed <!-- Draft | Proposed | Rejected | Accepted | Deprecated | Superseded by --></li>
<li>Deciders: V-Sekai</li>
<li>Tags: V-Sekai, Valve VR Ecosystem, Quest 3</li>
</ul>
</section>
<section id="the-backdrop" class="level2">
<h2 class="anchored" data-anchor-id="the-backdrop">The Backdrop</h2>
<p>The conversation between iFire and cyberpunkmermaid reveals an innovative idea of creating a portable VR setup. iFire has a cart with a PCVR computer in it. This setup streams to a Quest 3, which is a virtual reality headset. The system uses portable light houses on clamped sticks for tracking.</p>
</section>
<section id="the-challenge" class="level2">
<h2 class="anchored" data-anchor-id="the-challenge">The Challenge</h2>
<p>The challenge lies in integrating the power of the Valve VR ecosystem with the portability of the Quest series. It requires careful selection and arrangement of components to create a mobile VR station that can stream your desktop to your Meta Quest 2 or 3.</p>
</section>
<section id="the-strategy" class="level2">
<h2 class="anchored" data-anchor-id="the-strategy">The Strategy</h2>
<p>The strategy involves using 6 HTC or Tundra trackers motion capture. One tracker is dedicated to the continuous calibration with the Meta Quest 3 HMD and the SteamVR ecosystem. Two are used for the foot, two for the shoulders, one for the chest, one for the hips. The setup also includes 2 or 4 light houses and requires the purchase of Virtual Desktop for the Meta Quest 3.</p>
</section>
<section id="the-upside" class="level2">
<h2 class="anchored" data-anchor-id="the-upside">The Upside</h2>
<p>The upside of this setup is its portability and versatility. It essentially turns your laptop into a mobile VR station, allowing you to leverage the power of the Valve VR ecosystem anywhere you go.</p>
</section>
<section id="the-downside" class="level2">
<h2 class="anchored" data-anchor-id="the-downside">The Downside</h2>
<p>The downside could be the complexity of the setup and the cost of purchasing all the necessary components. However, the benefits of having a portable VR setup may outweigh these drawbacks for many users.</p>
</section>
<section id="the-road-not-taken" class="level2">
<h2 class="anchored" data-anchor-id="the-road-not-taken">The Road Not Taken</h2>
<p>An alternative approach could be to use a standalone VR headset like the Oculus Quest 2, but this would not provide the same level of performance and compatibility with the Valve VR ecosystem.</p>
</section>
<section id="the-infrequent-use-case" class="level2">
<h2 class="anchored" data-anchor-id="the-infrequent-use-case">The Infrequent Use Case</h2>
<p>In addition to regular use, the <a href="https://github.com/V-Sekai/V-Sekai-faceless">V-Sekai-faceless</a> repository on GitHub provides a solution for interacting with the virtual world without needing to wear the HMD (Head-Mounted Display). This can be beneficial in various scenarios such as testing or when the HMD is uncomfortable to wear for extended periods.</p>
</section>
<section id="in-core-and-done-by-us" class="level2">
<h2 class="anchored" data-anchor-id="in-core-and-done-by-us">In Core and Done by Us?</h2>
<p>This setup is not part of the core V-Sekai offering but is an innovative idea proposed by members of the community. It demonstrates the potential for creativity and innovation within the V-Sekai ecosystem.</p>
</section>
<section id="further-reading" class="level2">
<h2 class="anchored" data-anchor-id="further-reading">Further Reading</h2>
<ul>
<li>[V-Sekai] - AI assists this article.</li>
<li><a href="https://github.com/ArcticFox8515/OpenVR-SpaceCalibrator/releases/tag/v1.4-bd_%2Baf-r5">OpenVR-SpaceCalibrator</a></li>
<li><a href="https://github.com/V-Sekai/V-Sekai-faceless">V-Sekai-faceless</a></li>
</ul>


</section>
</section>

 ]]></description>
  <guid>https://v-sekai.github.io/manuals/decisions/20231019-quest3-mocap-steam-lighthouse.html</guid>
  <pubDate>Mon, 23 Oct 2023 22:52:52 GMT</pubDate>
</item>
<item>
  <title></title>
  <link>https://v-sekai.github.io/manuals/decisions/20230922-spacetime-shadow-shards-sync.html</link>
  <description><![CDATA[ 



<section id="streamlining-v-sekai-performance" class="level1">
<h1>Streamlining V-Sekai Performance</h1>
<section id="metadata" class="level2">
<h2 class="anchored" data-anchor-id="metadata">Metadata</h2>
<ul>
<li>Status: Proposed</li>
<li>Deciders: V-Sekai</li>
<li>Tags: V-Sekai, Godot, Networking</li>
</ul>
</section>
<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>V-Sekai, leveraging the Godot client-server model, aspires to support 1000-2000 clients. The objective is to deliver uninterrupted gameplay and a comprehensive gaming experience.</p>
</section>
<section id="challenges" class="level2">
<h2 class="anchored" data-anchor-id="challenges">Challenges</h2>
<p>Key challenges encompass managing single-shard overload, administering shadow shards, and implementing state synchronization and game state transitions. These tasks necessitate precise planning, execution, and monitoring for peak performance.</p>
</section>
<section id="approach" class="level2">
<h2 class="anchored" data-anchor-id="approach">Approach</h2>
<p>A simple gameplay session will be initiated where an art seller can upload artwork. The game environment will be a flat surface with a boundary, allowing mathematical movement calculations without a navigation mesh.</p>
<p>The number of clients connecting to the server will gradually increase, starting small and eventually reaching 2,000. This will test the server’s capacity to handle multiple connections. During testing, system resources will be monitored to estimate the cost and resources required to support 2000 players. Gameplay sessions will be recorded for the trade show, and load testing will be conducted to ensure our server can manage the maximum number of players without crashing or significant slowdowns.</p>
<ol type="1">
<li><strong>Server Configuration</strong>: Set up a Godot server that listens on a designated port.</li>
<li><strong>Client Development</strong>: Build a Godot client that connects to our server using High-Level Networking.</li>
<li><strong>Shadow Shard Administration</strong>: Generate shadow copies of the original shard, serialize them for network transmission, and synchronize their state periodically.</li>
<li><strong>State Synchronization</strong>: Develop <code>Ref&lt;Animation&gt;</code> data structures symbolizing different parts of the model and outline their interactions.</li>
<li><strong>Game State Transitions</strong>: Preserve, the current game state, pinpoint the target state, transition to it, and validate the transition using an orphan <code>AnimationTree</code>.</li>
</ol>
</section>
<section id="advantages" class="level2">
<h2 class="anchored" data-anchor-id="advantages">Advantages</h2>
<p>This strategy allows us to accommodate numerous clients simultaneously without sacrificing performance or user experience. It also provides a systematic method to monitor and enhance gaming sessions.</p>
</section>
<section id="limitations" class="level2">
<h2 class="anchored" data-anchor-id="limitations">Limitations</h2>
<p>This approach requires substantial resources and expertise in server configuration, networking, and game development. Unexpected challenges may surface during implementation and testing.</p>
</section>
<section id="alternatives" class="level2">
<h2 class="anchored" data-anchor-id="alternatives">Alternatives</h2>
<p>A potential alternative could have been adopting a different game engine or networking model. However, this would demand a complete revamp of our existing setup, possibly compromising performance and flexibility.</p>
</section>
<section id="exceptional-scenarios" class="level2">
<h2 class="anchored" data-anchor-id="exceptional-scenarios">Exceptional Scenarios</h2>
<p>Controversial plans are essential to minimize gameplay disruption in unexpected traffic surges or server outages.</p>
</section>
<section id="responsibility" class="level2">
<h2 class="anchored" data-anchor-id="responsibility">Responsibility</h2>
<p>All tasks outlined are integral to our operations and will be undertaken by us.</p>
</section>
<section id="additional-resources" class="level2">
<h2 class="anchored" data-anchor-id="additional-resources">Additional Resources</h2>
<ul>
<li>[V-Sekai]</li>
<li>AI assists this article.</li>
</ul>


</section>
</section>

 ]]></description>
  <guid>https://v-sekai.github.io/manuals/decisions/20230922-spacetime-shadow-shards-sync.html</guid>
  <pubDate>Mon, 23 Oct 2023 22:52:52 GMT</pubDate>
</item>
<item>
  <title></title>
  <link>https://v-sekai.github.io/manuals/decisions/20230920-godot-project-search.html</link>
  <description><![CDATA[ 



<section id="use-embedding-search-with-godot-engine-projects" class="level1">
<h1>Use embedding search with Godot Engine projects</h1>
<section id="metadata" class="level2">
<h2 class="anchored" data-anchor-id="metadata">Metadata</h2>
<ul>
<li>Status: proposed</li>
<li>Deciders: V-Sekai</li>
<li>Tags: V-Sekai</li>
</ul>
</section>
<section id="the-backdrop" class="level2">
<h2 class="anchored" data-anchor-id="the-backdrop">The Backdrop</h2>
<p>The Godot Engine is a powerful tool for creating games and other interactive content. However, finding specific pieces of code or resources within a large project can be challenging. This is where an embedding search engine like SeaGOAT can come in handy.</p>
</section>
<section id="the-challenge" class="level2">
<h2 class="anchored" data-anchor-id="the-challenge">The Challenge</h2>
<p>The challenge lies in integrating the SeaGOAT search engine with Godot Engine projects. Specifically, we need to ensure that it can effectively index and search <code>.tscn</code>, <code>.tres</code>, <code>.gd</code>, <code>.cpp</code>, <code>.h</code>, and <code>.xml</code> files, which are commonly used in Godot projects.</p>
</section>
<section id="the-strategy" class="level2">
<h2 class="anchored" data-anchor-id="the-strategy">The Strategy</h2>
<p>To use SeaGOAT with Godot Engine projects, you’ll need to add support for <code>.tscn</code>, <code>.tres</code>, <code>.gd</code>, <code>.cpp</code>, <code>.h</code>, and <code>.xml</code> files. You can do this by modifying the <code>common.py</code> file in the SeaGOAT repository. Specifically, you’ll want to add these extensions to the <code>FILE_EXTENSIONS</code> list:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1">FILE_EXTENSIONS <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb1-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># existing extensions...</span></span>
<span id="cb1-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">".tscn"</span>,</span>
<span id="cb1-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">".tres"</span>,</span>
<span id="cb1-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">".gd"</span>,</span>
<span id="cb1-6">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">".cpp"</span>,</span>
<span id="cb1-7">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">".h"</span>,</span>
<span id="cb1-8">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">".xml"</span></span>
<span id="cb1-9">]</span></code></pre></div>
<p>Once you’ve done this, SeaGOAT should be able to index and search your Godot Engine projects.</p>
</section>
<section id="the-upside" class="level2">
<h2 class="anchored" data-anchor-id="the-upside">The Upside</h2>
<p>By integrating SeaGOAT with your Godot Engine projects, you’ll be able to quickly and easily find specific pieces of code or resources. This can greatly speed up your development process and make it easier to manage large projects.</p>
</section>
<section id="the-downside" class="level2">
<h2 class="anchored" data-anchor-id="the-downside">The Downside</h2>
<p>The downside is that this requires modifying the SeaGOAT source code, which may not be ideal if you’re not comfortable with Python or if you want to keep your SeaGOAT installation up-to-date with the official repository.</p>
</section>
<section id="the-road-not-taken" class="level2">
<h2 class="anchored" data-anchor-id="the-road-not-taken">The Road Not Taken</h2>
<p>An alternative approach would be to use a different search engine that already supports <code>.tscn</code>, <code>.tres</code>, <code>.gd</code>, <code>.cpp</code>, <code>.h</code>, and <code>.xml</code> files. However, this may not provide the same level of functionality as SeaGOAT.</p>
</section>
<section id="the-infrequent-use-case" class="level2">
<h2 class="anchored" data-anchor-id="the-infrequent-use-case">The Infrequent Use Case</h2>
<p>This strategy is most useful for large Godot Engine projects where finding specific pieces of code or resources can be challenging. For smaller projects, the built-in search functionality in Godot may be sufficient.</p>
</section>
<section id="in-core-and-done-by-us" class="level2">
<h2 class="anchored" data-anchor-id="in-core-and-done-by-us">In Core and Done by Us?</h2>
<p>This modification would need to be done by us, as it involves changing the SeaGOAT source code. It’s not clear whether this change would be accepted into the core SeaGOAT project.</p>
</section>
<section id="further-reading" class="level2">
<h2 class="anchored" data-anchor-id="further-reading">Further Reading</h2>
<ul>
<li><a href="https://v-sekai.org/">V-Sekai</a></li>
<li><a href="https://github.com/kantord/SeaGOAT">SeaGOAT</a></li>
<li>This article is assisted by AI.</li>
</ul>


</section>
</section>

 ]]></description>
  <guid>https://v-sekai.github.io/manuals/decisions/20230920-godot-project-search.html</guid>
  <pubDate>Mon, 23 Oct 2023 22:52:52 GMT</pubDate>
</item>
<item>
  <title></title>
  <link>https://v-sekai.github.io/manuals/decisions/20230913-V-Sekai-bugs.html</link>
  <description><![CDATA[ 



<section id="challenges-that-need-to-be-addressed-in-v-sekai" class="level1">
<h1>Challenges that need to be addressed in V-Sekai</h1>
<section id="metadata" class="level2">
<h2 class="anchored" data-anchor-id="metadata">Metadata</h2>
<ul>
<li><strong>Status:</strong> Proposed</li>
<li><strong>Deciders:</strong> V-Sekai</li>
<li><strong>Tags:</strong> V-Sekai</li>
</ul>
</section>
<section id="the-backdrop" class="level2">
<h2 class="anchored" data-anchor-id="the-backdrop">The Backdrop</h2>
<p>In our quest to expand our community, we are faced with several challenges that need to be addressed.</p>
</section>
<section id="the-challenge" class="level2">
<h2 class="anchored" data-anchor-id="the-challenge">The Challenge</h2>
<p>Our game has been QA Engineered by Facade, which was a great help. However, there are still issues that need to be resolved.</p>
</section>
<section id="the-strategy" class="level2">
<h2 class="anchored" data-anchor-id="the-strategy">The Strategy</h2>
<p>We have identified several areas of concern and have charted them in the following flow diagram:</p>
<pre class="mermaid"><code>graph TD
    Crash["V-Sekai crash on start on RADV, Ubuntu 22.04.2 LTS (Jammy Jellyfish) #115"]
    VR["VR doesn't work #228"]
    Avatar["Restore avatar upload #243"]
    Uploads["Restore world uploads #244"]
    Mirror["Restore mirror in the preview server #249"]
    Freeze["Pressing escape button while ingame freezes client #250"]
    Escape["Escape button in menu should go back, instead does nothing #251"]
    Text["UI dropdown text is invisible until hovered #252"]
    Sensitivity["Increase default mouse sensitivity #253"]
    Gestures["Restore gestures #254"]
    VOIP["Restore VOIP #255"]
    IK["IK errors ingame #256"]

    Crash--&gt;VR
    VR--&gt;Avatar
    Avatar--&gt;Uploads
    Uploads--&gt;Mirror
    Mirror--&gt;Freeze
    Freeze--&gt;Escape
    Escape--&gt;Text
    Text--&gt;Sensitivity
    Sensitivity--&gt;Gestures
    Gestures--&gt;VOIP
    VOIP--&gt;IK</code></pre>
</section>
<section id="the-upside" class="level2">
<h2 class="anchored" data-anchor-id="the-upside">The Upside</h2>
<p>Addressing these issues will significantly improve the user experience, making our game more enjoyable and accessible. This could lead to an increase in our user base and community engagement.</p>
</section>
<section id="the-downside" class="level2">
<h2 class="anchored" data-anchor-id="the-downside">The Downside</h2>
<p>The process of resolving these issues may require significant time and resources. It might also involve some trial and error, which could potentially introduce new bugs or problems.</p>
</section>
<section id="the-road-not-taken" class="level2">
<h2 class="anchored" data-anchor-id="the-road-not-taken">The Road Not Taken</h2>
<p>Ignoring these issues is not an option as it would negatively impact the user experience and could hinder the growth of our community.</p>
</section>
<section id="the-infrequent-use-case" class="level2">
<h2 class="anchored" data-anchor-id="the-infrequent-use-case">The Infrequent Use Case</h2>
<p>While some of these issues might only affect a small number of users or occur under specific circumstances, it’s important to address them to ensure a smooth and enjoyable experience for all users.</p>
</section>
<section id="in-core-and-done-by-us" class="level2">
<h2 class="anchored" data-anchor-id="in-core-and-done-by-us">In Core and Done by Us?</h2>
<p>Yes, these issues are core to the functionality of our game and should be addressed by our team.</p>
</section>
<section id="further-reading" class="level2">
<h2 class="anchored" data-anchor-id="further-reading">Further Reading</h2>
<ul>
<li><a href="https://v-sekai.org/">V-Sekai</a></li>
<li>This article is assisted by AI.</li>
</ul>


</section>
</section>

 ]]></description>
  <guid>https://v-sekai.github.io/manuals/decisions/20230913-V-Sekai-bugs.html</guid>
  <pubDate>Mon, 23 Oct 2023 22:52:52 GMT</pubDate>
</item>
<item>
  <title></title>
  <link>https://v-sekai.github.io/manuals/decisions/20230912-target-most-popular-resolution.html</link>
  <description><![CDATA[ 



<section id="address-screen-resolution-compatibility-issues" class="level1">
<h1>Address Screen Resolution Compatibility Issues</h1>
<section id="metadata" class="level2">
<h2 class="anchored" data-anchor-id="metadata">Metadata</h2>
<ul>
<li><strong>Status:</strong> Proposed</li>
<li><strong>Deciders:</strong> V-Sekai</li>
<li><strong>Tags:</strong> V-Sekai</li>
</ul>
</section>
<section id="context-and-problem-statement" class="level2">
<h2 class="anchored" data-anchor-id="context-and-problem-statement">Context and Problem Statement</h2>
<p>Several users, including Lyuma, have reported that their monitors are not compatible with the V-Sekai menu system. This issue is causing a significant hindrance in the user experience.</p>
</section>
<section id="decision-drivers" class="level2">
<h2 class="anchored" data-anchor-id="decision-drivers">Decision Drivers</h2>
<ul>
<li>User satisfaction</li>
<li>Compatibility with various screen resolutions</li>
</ul>
</section>
<section id="considered-options" class="level2">
<h2 class="anchored" data-anchor-id="considered-options">Considered Options</h2>
<ol type="1">
<li>Leave as is</li>
<li>Set V-Sekai’s screen resolution to fit the default resolution of the Valve hardware survey’s most popular resolution.</li>
</ol>
</section>
<section id="decision-outcome" class="level2">
<h2 class="anchored" data-anchor-id="decision-outcome">Decision Outcome</h2>
<p>Chosen option: “Set V-Sekai’s screen resolution to fit the default resolution of the Valve hardware survey’s most popular resolution.”, because it will increase compatibility and improve user experience.</p>
</section>
<section id="pros-and-cons-of-the-options" class="level2">
<h2 class="anchored" data-anchor-id="pros-and-cons-of-the-options">Pros and Cons of the Options</h2>
<section id="setting-v-sekais-screen-resolution-to-fit-the-default-resolution-of-the-valve-hardware-surveys-most-popular-resolution" class="level3">
<h3 class="anchored" data-anchor-id="setting-v-sekais-screen-resolution-to-fit-the-default-resolution-of-the-valve-hardware-surveys-most-popular-resolution">Setting V-Sekai’s Screen Resolution to Fit the Default Resolution of the Valve Hardware Survey’s Most Popular Resolution</h3>
<ul>
<li>Good, because it increases compatibility with the majority of users’ monitors.</li>
<li>Bad, because it may not cover all possible monitor resolutions, leaving some users still facing issues.</li>
</ul>
</section>
<section id="leaving-as-is" class="level3">
<h3 class="anchored" data-anchor-id="leaving-as-is">Leaving as Is</h3>
<ul>
<li>Good, because it requires no additional work or changes.</li>
<li>Bad, because it does not solve the problem and leaves users frustrated.</li>
</ul>
</section>
</section>
<section id="links" class="level2">
<h2 class="anchored" data-anchor-id="links">Links</h2>
<ul>
<li><a href="https://v-sekai.org/">V-Sekai</a></li>
<li>This article is assisted by AI.</li>
</ul>


</section>
</section>

 ]]></description>
  <guid>https://v-sekai.github.io/manuals/decisions/20230912-target-most-popular-resolution.html</guid>
  <pubDate>Mon, 23 Oct 2023 22:52:52 GMT</pubDate>
</item>
<item>
  <title></title>
  <link>https://v-sekai.github.io/manuals/decisions/20230911-xr-grid-with-drawing.html</link>
  <description><![CDATA[ 



<section id="improving-xr-grid-with-new-enhancements" class="level1">
<h1>Improving xr grid with new enhancements</h1>
<section id="metadata" class="level2">
<h2 class="anchored" data-anchor-id="metadata">Metadata</h2>
<ul>
<li><strong>Status:</strong> proposed</li>
<li><strong>Deciders:</strong> V-Sekai, fire, detox,</li>
<li><strong>Tags:</strong> V-Sekai</li>
</ul>
</section>
<section id="context-and-problem-statement" class="level2">
<h2 class="anchored" data-anchor-id="context-and-problem-statement">Context and Problem Statement</h2>
<p>We are developing an xr grid project, but it’s in a nice easy to understand state.</p>
</section>
<section id="considered-options" class="level2">
<h2 class="anchored" data-anchor-id="considered-options">Considered Options</h2>
<p>DETOX suggested comparing different modes of navigation before making any changes. Fire shared that they have three modes in mind: blob pen, 2D beautifications for 3D, and patch mode. However, these are not functioning correctly yet due to complexity.</p>
</section>
<section id="decision-outcome" class="level2">
<h2 class="anchored" data-anchor-id="decision-outcome">Decision Outcome</h2>
<p>Fire will think up some UI for xr-grid, create a simple UI system based on GUI in VR, and add it to xr-grid. We also discuss the possibility of adding a glTF export menu and other features to the UI.</p>
</section>
<section id="positive-consequences" class="level2">
<h2 class="anchored" data-anchor-id="positive-consequences">Positive Consequences</h2>
<p>The team is considering various improvements such as fixing grid bugs, exporting loadable content in Blender, adding branding/instructions, and improving the UI.</p>
</section>
<section id="negative-consequences" class="level2">
<h2 class="anchored" data-anchor-id="negative-consequences">Negative Consequences</h2>
<p>The main challenge is to avoid bloating the main project with art tools. This might require branching or forking the project.</p>
</section>
<section id="prospects-for-the-future" class="level2">
<h2 class="anchored" data-anchor-id="prospects-for-the-future">Prospects for the Future</h2>
<p>DETOX suggested creating a branch for the new features, while Fire expressed a desire to work on networking. They also discussed the possibility of building a tool similar to Cassie on the branch.</p>
</section>
<section id="links" class="level2">
<h2 class="anchored" data-anchor-id="links">Links</h2>
<ul>
<li><a href="https://v-sekai.org/">V-Sekai</a></li>
<li>This article is assisted by AI.</li>
</ul>


</section>
</section>

 ]]></description>
  <guid>https://v-sekai.github.io/manuals/decisions/20230911-xr-grid-with-drawing.html</guid>
  <pubDate>Mon, 23 Oct 2023 22:52:52 GMT</pubDate>
</item>
<item>
  <title></title>
  <link>https://v-sekai.github.io/manuals/decisions/20230911-first-alpha-tag.html</link>
  <description><![CDATA[ 



<section id="implementing-semantic-versioning-for-alpha-releases-in-v-sekai" class="level1">
<h1>Implementing Semantic Versioning for Alpha Releases in V-Sekai</h1>
<section id="metadata" class="level2">
<h2 class="anchored" data-anchor-id="metadata">Metadata</h2>
<ul>
<li>Status: Proposed <!-- Draft | Proposed | Rejected | Accepted | Deprecated | Superseded by --></li>
<li>Deciders: V-Sekai</li>
<li>Tags: V-Sekai, Semantic Versioning, Alpha Release</li>
</ul>
</section>
<section id="the-backdrop" class="level2">
<h2 class="anchored" data-anchor-id="the-backdrop">The Backdrop</h2>
<p>In software development, semantic versioning (semver) is a popular method for versioning software. It provides a clear and concise way to communicate changes in versions and the associated implications.</p>
</section>
<section id="the-challenge" class="level2">
<h2 class="anchored" data-anchor-id="the-challenge">The Challenge</h2>
<p>The challenge lies in correctly denoting the first alpha release in semver. This is crucial as it communicates the stability of the software and sets expectations for users and developers.</p>
</section>
<section id="the-strategy" class="level2">
<h2 class="anchored" data-anchor-id="the-strategy">The Strategy</h2>
<p>In semver, the first alpha release would typically be denoted with a hyphen followed by the word “alpha” and a number. For example, it could be something like “0.1.0-alpha.1”. The specific alpha release number may vary depending on the project and its release cycle.</p>
</section>
<section id="the-upside" class="level2">
<h2 class="anchored" data-anchor-id="the-upside">The Upside</h2>
<p>Using semver for alpha releases helps to set clear expectations about the stability and features of the software. It allows developers and users to understand that the software is in an early stage of development and may not yet have all planned features or be fully stable.</p>
</section>
<section id="the-downside" class="level2">
<h2 class="anchored" data-anchor-id="the-downside">The Downside</h2>
<p>The downside could be the potential confusion around the naming convention, especially for those unfamiliar with semver. However, this can be mitigated through clear documentation and communication.</p>
</section>
<section id="the-road-not-taken" class="level2">
<h2 class="anchored" data-anchor-id="the-road-not-taken">The Road Not Taken</h2>
<p>An alternative approach could be to use a different versioning system or to not denote alpha releases specifically. However, these approaches may not provide the same level of clarity and communication as semver.</p>
</section>
<section id="the-infrequent-use-case" class="level2">
<h2 class="anchored" data-anchor-id="the-infrequent-use-case">The Infrequent Use Case</h2>
<p>While the alpha release notation is common in semver, it may not be used frequently in projects that have shorter development cycles or that do not release alpha versions to the public.</p>
</section>
<section id="in-core-and-done-by-us" class="level2">
<h2 class="anchored" data-anchor-id="in-core-and-done-by-us">In Core and Done by Us?</h2>
<p>Yes, this strategy of using semver for denoting alpha releases is a core part of the V-Sekai project management and release process.</p>
</section>
<section id="further-reading" class="level2">
<h2 class="anchored" data-anchor-id="further-reading">Further Reading</h2>
<ul>
<li>[V-Sekai] - AI assists this article.</li>
<li><a href="https://semver.org/">Semantic Versioning</a></li>
</ul>


</section>
</section>

 ]]></description>
  <guid>https://v-sekai.github.io/manuals/decisions/20230911-first-alpha-tag.html</guid>
  <pubDate>Mon, 23 Oct 2023 22:52:52 GMT</pubDate>
</item>
<item>
  <title></title>
  <link>https://v-sekai.github.io/manuals/decisions/20230908-bendable-screen-generative-assistant-ggml.html</link>
  <description><![CDATA[ 



<section id="translationtranscription-badge-with-ggmls-whisper" class="level1">
<h1>Translation/Transcription Badge with GGML’s Whisper</h1>
<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview">Overview</h2>
<p>The project aims to develop a translation/transcription badge using GGML’s Whisper, a part of the GGML library that provides access to large language models (LLMs). The primary focus is on voice interactions.</p>
</section>
<section id="key-features" class="level2">
<h2 class="anchored" data-anchor-id="key-features">Key Features</h2>
<ul>
<li><strong>Voice Interactions</strong>: The system will prioritize voice interactions, leveraging GGML’s Whisper capabilities to create a responsive AI. Python will not be used in this context.</li>
<li><strong>Speech-to-Text Interface</strong>: A speech-to-text interface will be incorporated to process voice inputs effectively.</li>
<li><strong>Direct Connection to Compute Module</strong>: The display, set in portrait mode, will connect directly to the compute module, a standard Linux amd64 computer, eliminating the need for embedded software development.</li>
<li><strong>Flexible OLED Display</strong>: The system will utilize a 6-inch flexible OLED display from Wisecoco with a resolution of 2880*1440 at 60Hz refresh rate.</li>
<li><strong>No Holograph Displays</strong>: The design will be streamlined and efficient, excluding holograph displays.</li>
</ul>
</section>
<section id="potential-challenges" class="level2">
<h2 class="anchored" data-anchor-id="potential-challenges">Potential Challenges</h2>
<p>The project’s complexity and the expertise required in areas such as GGML, the Godot engine, and AI development pose significant challenges.</p>
</section>
<section id="alternatives-not-considered" class="level2">
<h2 class="anchored" data-anchor-id="alternatives-not-considered">Alternatives Not Considered</h2>
<p>We have decided against integrating Python with GGML and using a pre-existing AI framework instead of developing one from scratch.</p>
</section>
<section id="target-audience" class="level2">
<h2 class="anchored" data-anchor-id="target-audience">Target Audience</h2>
<p>The project may not be suitable for individuals without familiarity with GGML or a background in AI development.</p>
</section>
<section id="development-strategy" class="level2">
<h2 class="anchored" data-anchor-id="development-strategy">Development Strategy</h2>
<p>The project will be developed in-house, utilizing our expertise in AI and game development.</p>
</section>
<section id="further-reading" class="level2">
<h2 class="anchored" data-anchor-id="further-reading">Further Reading</h2>
<p>For more information, please refer to these resources:</p>
<ul>
<li><a href="https://v-sekai.org/">V-Sekai</a></li>
<li><a href="https://github.com/georgi-gerganov/ggml">GGML on GitHub</a></li>
<li><a href="https://github.com/sanchit-gandhi/whisper-jax">Whisper JAX on GitHub</a></li>
<li><a href="https://twitter.com/jav6868/status/1698260873352212662?s=20">Twitter inspiration</a></li>
<li><a href="https://www.amazon.ca/wisecoco-Flexible-Display-Screen-Bendable/dp/B0C7YY16Z5">wisecoco 6 inch 2K Flexible OLED Display IPS 2880 * 1440 Flexible Screen Curved Bendable Screen</a></li>
</ul>
</section>
<section id="system-diagram" class="level2">
<h2 class="anchored" data-anchor-id="system-diagram">System Diagram</h2>
<pre class="mermaid"><code>graph TD
A[Edge Device: Compute Unit] --&gt;|Hosts| B[VRM1 Character Layer]
C[Whisper for Speech-to-Text running on GGML] -- Audio Processing --&gt; B
B --&gt; D[Wisecoco 6-inch Flexible OLED Display in Portrait Mode]
E[Microphone] -- Audio Input --&gt; C</code></pre>
</section>
<section id="system-diagram-explanation" class="level2">
<h2 class="anchored" data-anchor-id="system-diagram-explanation">System Diagram Explanation:</h2>
<section id="edge-device-compute-unit-a" class="level3">
<h3 class="anchored" data-anchor-id="edge-device-compute-unit-a">1. Edge Device: Compute Unit (A)</h3>
<p>This unit is a standard Linux amd64 computer that hosts the VRM1 Character Layer (B).</p>
</section>
<section id="whisper-for-speech-to-text-module-c" class="level3">
<h3 class="anchored" data-anchor-id="whisper-for-speech-to-text-module-c">2. Whisper for Speech-to-Text module (C)</h3>
<p>This module, running on GGML, captures audio input from the Microphone (E) and converts it into text.</p>
</section>
<section id="vrm1-character-layer-b" class="level3">
<h3 class="anchored" data-anchor-id="vrm1-character-layer-b">3. VRM1 Character Layer (B)</h3>
<p>The processed data from the Whisper module animates a virtual character in this layer. The character can be in speaking or idle mode.</p>
</section>
<section id="wisecoco-6-inch-flexible-oled-display-in-portrait-mode-d" class="level3">
<h3 class="anchored" data-anchor-id="wisecoco-6-inch-flexible-oled-display-in-portrait-mode-d">4. Wisecoco 6-inch Flexible OLED Display in Portrait Mode (D)</h3>
<p>The resulting texts are displayed on this screen.</p>
<blockquote class="blockquote">
<p><strong>Note:</strong> Currently, the system only supports speaking and idle states. Other modes are not available.</p>
</blockquote>
<p>The choice of portrait mode for the display aligns with the specific needs and constraints of the project. As the project heavily relies on text-based interactions, specifically transcriptions, portrait mode allows more lines of text to be visible at once, thereby improving the user experience.</p>
<p>The system utilizes a 6-inch flexible OLED display from Wisecoco. Portrait mode is more space-efficient in such scenarios, taking up less horizontal space while still providing ample vertical space for text display.</p>
<p>Given that the primary focus of the project is voice interactions, the display mode should ideally support easy reading of transcriptions. Portrait mode, with its vertical orientation, is more suited to this task as it mimics the natural top-to-bottom reading flow.</p>
<p>According to the system diagram, the processed data from the Whisper for Speech-to-Text module running on GGML is used to animate a virtual character and display the resulting texts. These texts are likely to be dialogue or conversation-based, and portrait mode would allow for a more coherent and continuous display of these text blocks.</p>


</section>
</section>
</section>

 ]]></description>
  <guid>https://v-sekai.github.io/manuals/decisions/20230908-bendable-screen-generative-assistant-ggml.html</guid>
  <pubDate>Mon, 23 Oct 2023 22:52:52 GMT</pubDate>
</item>
<item>
  <title></title>
  <link>https://v-sekai.github.io/manuals/decisions/20230903-replicate-vrm-transform.html</link>
  <description><![CDATA[ 



<section id="enabling-vrm-to-blender-conversion-with-cloud-gpus-and-replicate.com" class="level1">
<h1>Enabling VRM to Blender Conversion with Cloud GPUs and replicate.com</h1>
<section id="metadata" class="level2">
<h2 class="anchored" data-anchor-id="metadata">Metadata</h2>
<ul>
<li>Status: rejected</li>
<li>Deciders: V-Sekai, fire, antpb</li>
<li>Tags: V-Sekai</li>
</ul>
</section>
<section id="the-backdrop" class="level2">
<h2 class="anchored" data-anchor-id="the-backdrop">The Backdrop</h2>
<p>The VRM format is becoming increasingly popular in the field of 3D modeling and animation. It allows for easy sharing and transfer of humanoid 3D models with skeletal animations between different software applications. One such application is the Godot Engine, a popular open-source game engine.</p>
</section>
<section id="the-challenge" class="level2">
<h2 class="anchored" data-anchor-id="the-challenge">The Challenge</h2>
<p>Currently, developers working with VRM files in Godot Engine face difficulties when trying to convert these files to Blender format for further editing or manipulation. Manual conversion processes can be time-consuming and error-prone.</p>
</section>
<section id="the-strategy" class="level2">
<h2 class="anchored" data-anchor-id="the-strategy">The Strategy</h2>
<p>The proposed strategy for enabling VRM to Blender conversion involves leveraging the computational power of cloud GPUs provided by replicate.com. By running a GPU-accelerated Blender instance, developers can perform an identity operation on the VRM input, converting it to Blender format. This simplifies the process of converting VRM files to Blender format.</p>
<p>To convert VRM files to Blender format, developers can follow these steps:</p>
<ol type="1">
<li>Install the necessary libraries or plugins in Blender that support the VRM format.</li>
<li>Leverage replicate.com’s infrastructure to spin up a GPU-accelerated Blender instance.</li>
<li>Develop a script or tool that takes the VRM file as input and sends it to the GPU-accelerated Blender instance via a REST interface.</li>
<li>In the GPU-accelerated Blender instance, use the installed libraries or plugins to perform the conversion from VRM to Blender format.</li>
<li>Also use Blender to perform the conversion from Blender to VRM/GLTF format.</li>
</ol>
<p>This strategy streamlines the workflow for developers, saving time and improving collaboration between designers and developers. The use of cloud GPUs and replicate.com’s infrastructure ensures fast and efficient conversion processes.</p>
<p>Additionally, this strategy can also be extended to other software applications or websites that can utilize a REST interface, allowing developers working with VRM files to convert them to Blender format seamlessly, regardless of the software they are using.</p>
</section>
<section id="the-upside" class="level2">
<h2 class="anchored" data-anchor-id="the-upside">The Upside</h2>
<ul>
<li>Streamlined workflow: Developers can easily convert VRM files to Blender format, opening up a wide range of editing and manipulation possibilities.</li>
<li>Time-saving: The use of cloud GPUs and replicate.com’s infrastructure ensures fast and efficient conversion processes.</li>
<li>Improved collaboration: The ability to work with VRM models in both Godot Engine and Blender facilitates collaboration between designers and developers.</li>
</ul>
</section>
<section id="the-downside" class="level2">
<h2 class="anchored" data-anchor-id="the-downside">The Downside</h2>
<ul>
<li>Development effort and expertise: Creating the necessary library or plugin may require significant technical knowledge in both the Godot Engine and VRM format.</li>
<li>Compatibility issues: Different versions of FBX or GLB files may introduce compatibility challenges during the conversion process.</li>
<li>Maintenance challenges: Keeping up with future updates to the Godot Engine and VRM specifications may require ongoing effort.</li>
</ul>
</section>
<section id="the-road-not-taken" class="level2">
<h2 class="anchored" data-anchor-id="the-road-not-taken">The Road Not Taken</h2>
<p>We considered developing a web application, but it was deemed to have heavy engineering costs.</p>
</section>
<section id="the-infrequent-use-case" class="level2">
<h2 class="anchored" data-anchor-id="the-infrequent-use-case">The Infrequent Use Case</h2>
<p>While the proposed solution focuses on streamlining VRM to Blender conversion for the Godot Engine ecosystem, the same concept can be extended to other software applications or websites that can utilize a REST interface. This allows developers working with VRM files to convert them to Blender format seamlessly, regardless of the software they are using. The use of cloud GPUs and replicate.com’s infrastructure still ensures fast and efficient conversion processes.</p>
</section>
<section id="in-core-and-done-by-us" class="level2">
<h2 class="anchored" data-anchor-id="in-core-and-done-by-us">In Core and Done by Us?</h2>
<p>Fire knows the way.</p>
</section>
<section id="further-reading" class="level2">
<h2 class="anchored" data-anchor-id="further-reading">Further Reading</h2>
<ul>
<li><a href="https://v-sekai.org/">V-Sekai</a></li>
<li><a href="https://github.com/vrm-c/vrm-specification/tree/master/specification/VRMC_vrm_animation-1.0">VRM Animation Specification</a></li>
<li><a href="https://replicate.com/fire/v-sekai.mediapipe-labeler">Replicate - Cloud GPU Toy</a></li>
</ul>
<p>This article is assisted by AI.</p>
<p>Is there anything else I can assist you with?</p>


</section>
</section>

 ]]></description>
  <guid>https://v-sekai.github.io/manuals/decisions/20230903-replicate-vrm-transform.html</guid>
  <pubDate>Mon, 23 Oct 2023 22:52:52 GMT</pubDate>
</item>
<item>
  <title></title>
  <link>https://v-sekai.github.io/manuals/decisions/20230831-fixing-upload-limits.html</link>
  <description><![CDATA[ 



<section id="enhancing-map-uploads" class="level1">
<h1>Enhancing Map Uploads</h1>
<section id="metadata" class="level2">
<h2 class="anchored" data-anchor-id="metadata">Metadata</h2>
<ul>
<li>Status: accepted</li>
<li>Deciders: V-Sekai, fire, Scipi</li>
<li>Tags: V-Sekai</li>
</ul>
</section>
<section id="the-backdrop" class="level2">
<h2 class="anchored" data-anchor-id="the-backdrop">The Backdrop</h2>
<p>V-Sekai is facing limitations in its Elixir uro backend when uploading maps, specifically related to the Godot Engine scenes. These limitations are believed to be caused by a restriction in the Waffles library.</p>
</section>
<section id="the-challenge" class="level2">
<h2 class="anchored" data-anchor-id="the-challenge">The Challenge</h2>
<p>The challenge at hand is to address the arbitrary limitations imposed by the Waffles library on the Godot Engine scenes during map uploads in the Elixir uro backend of V-Sekai.</p>
</section>
<section id="the-strategy" class="level2">
<h2 class="anchored" data-anchor-id="the-strategy">The Strategy</h2>
<p>To overcome this challenge, we propose patching the Waffles library in collaboration with our new contributor.</p>
</section>
<section id="the-upside" class="level2">
<h2 class="anchored" data-anchor-id="the-upside">The Upside</h2>
<p>By patching the Waffles library, we can effectively remove the limitations on the Godot Engine scenes and improve the functionality of map uploads in V-Sekai.</p>
</section>
<section id="the-downside" class="level2">
<h2 class="anchored" data-anchor-id="the-downside">The Downside</h2>
<p>It is important to consider any potential downsides or risks associated with patching the Waffles library. Further analysis and testing may be required to ensure compatibility and stability.</p>
</section>
<section id="the-road-not-taken" class="level2">
<h2 class="anchored" data-anchor-id="the-road-not-taken">The Road Not Taken</h2>
<p>Alternative solutions to address the limitations have been explored, but patching the Waffles library appears to be the most viable option.</p>
</section>
<section id="the-infrequent-use-case" class="level2">
<h2 class="anchored" data-anchor-id="the-infrequent-use-case">The Infrequent Use Case</h2>
<p>This limitation impacts the specific use case of map uploads in the Elixir uro backend, and may not affect other functionalities in V-Sekai.</p>
</section>
<section id="in-core-and-done-by-us" class="level2">
<h2 class="anchored" data-anchor-id="in-core-and-done-by-us">In Core and Done by Us?</h2>
<p>As a core functionality of V-Sekai, addressing the limitations in the Godot Engine scenes during map uploads is a task that should be done by our team.</p>
</section>
<section id="further-reading" class="level2">
<h2 class="anchored" data-anchor-id="further-reading">Further Reading</h2>
<ul>
<li><a href="https://v-sekai.org/">V-Sekai</a></li>
<li>This article is assisted by AI.</li>
<li>https://github.com/V-Sekai/v-sekai-game/issues/68</li>
</ul>


</section>
</section>

 ]]></description>
  <guid>https://v-sekai.github.io/manuals/decisions/20230831-fixing-upload-limits.html</guid>
  <pubDate>Mon, 23 Oct 2023 22:52:52 GMT</pubDate>
</item>
<item>
  <title></title>
  <link>https://v-sekai.github.io/manuals/decisions/20230831-fixing-menus.html</link>
  <description><![CDATA[ 



<section id="fix-menus-in-game" class="level1">
<h1>Fix menus in-game</h1>
<section id="metadata" class="level2">
<h2 class="anchored" data-anchor-id="metadata">Metadata</h2>
<ul>
<li>Status: accepted</li>
<li>Deciders: V-Sekai</li>
<li>Tags: V-Sekai</li>
</ul>
</section>
<section id="the-backdrop" class="level2">
<h2 class="anchored" data-anchor-id="the-backdrop">The Backdrop</h2>
</section>
<section id="the-challenge" class="level2">
<h2 class="anchored" data-anchor-id="the-challenge">The Challenge</h2>
<p>The menus in-game are unable to capture events from the VR controllers.</p>
</section>
<section id="the-strategy" class="level2">
<h2 class="anchored" data-anchor-id="the-strategy">The Strategy</h2>
<p>Debug the <code>on_pointer_pressed</code> function as it is not working correctly due to an issue with <code>Transform3D.xform_inv</code>. The current implementation mimics mouse input, but it fails to translate global positions correctly. Review the <code>global_to_viewport</code> function and ensure it returns the correct value for <code>local_at</code>.</p>
</section>
<section id="the-upside" class="level2">
<h2 class="anchored" data-anchor-id="the-upside">The Upside</h2>
<p>Fixing the menus will enable players to interact with them using VR controllers, enhancing the user experience.</p>
</section>
<section id="the-downside" class="level2">
<h2 class="anchored" data-anchor-id="the-downside">The Downside</h2>
<p>Fixing the menus may require significant code changes, introducing potential bugs or conflicts that need proper testing to ensure stability.</p>
</section>
<section id="the-road-not-taken" class="level2">
<h2 class="anchored" data-anchor-id="the-road-not-taken">The Road Not Taken</h2>
<p>Exploring alternative input systems specifically designed for VR controllers could be considered, but it may involve rewriting a large portion of the menu code and potentially introducing new dependencies.</p>
</section>
<section id="the-infrequent-use-case" class="level2">
<h2 class="anchored" data-anchor-id="the-infrequent-use-case">The Infrequent Use Case</h2>
<p>This fix targets players using VR controllers to interact with the menus. Players without VR controllers will not be affected.</p>
</section>
<section id="in-core-and-done-by-us" class="level2">
<h2 class="anchored" data-anchor-id="in-core-and-done-by-us">In Core and Done by Us?</h2>
<p>Yes, this issue falls within our core development responsibilities, and we will take ownership of fixing the menus.</p>
</section>
<section id="further-reading" class="level2">
<h2 class="anchored" data-anchor-id="further-reading">Further Reading</h2>
<ul>
<li><a href="https://v-sekai.org/">V-Sekai</a></li>
<li>This article is assisted by AI.</li>
</ul>


</section>
</section>

 ]]></description>
  <guid>https://v-sekai.github.io/manuals/decisions/20230831-fixing-menus.html</guid>
  <pubDate>Mon, 23 Oct 2023 22:52:52 GMT</pubDate>
</item>
<item>
  <title></title>
  <link>https://v-sekai.github.io/manuals/decisions/20230830-generating-v-sekai-worlds.html</link>
  <description><![CDATA[ 



<section id="expanding-worlds" class="level1">
<h1>Expanding Worlds</h1>
<section id="metadata" class="level2">
<h2 class="anchored" data-anchor-id="metadata">Metadata</h2>
<ul>
<li>Status: proposed</li>
<li>Deciders: V-Sekai</li>
<li>Tags: V-Sekai</li>
</ul>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>V-Sekai is a virtual reality (VR) platform that offers users the opportunity to immerse themselves in virtual worlds. However, there are certain limitations that hinder the overall user experience. This proposal aims to address these limitations by proposing specific areas for improvement within V-Sekai.</p>
</section>
<section id="the-challenge" class="level2">
<h2 class="anchored" data-anchor-id="the-challenge">The Challenge</h2>
<p>V-Sekai currently faces several limitations, including non-functional menus in VR mode and a limited number of available worlds. These challenges affect the usability and variety of experiences for the users. To overcome these challenges and enhance the V-Sekai user experience, we must prioritize and focus on one area for improvement.</p>
<section id="proposed-solutions" class="level3">
<h3 class="anchored" data-anchor-id="proposed-solutions">Proposed Solutions</h3>
<p>We propose focusing on <strong>Generating More Worlds</strong> as the first step towards enhancing V-Sekai. By creating new VR worlds within V-Sekai, users will have a wider range of experiences to explore. This involves designing and implementing new environments to expand the options available. With more worlds to choose from, users will have increased engagement and longer sessions within the V-Sekai platform.</p>
</section>
</section>
<section id="the-strategy" class="level2">
<h2 class="anchored" data-anchor-id="the-strategy">The Strategy</h2>
<p>To generate more worlds for V-Sekai, we will follow a comprehensive strategy that includes the following steps:</p>
<ol type="1">
<li><p><strong>Identify User Preferences</strong>: Understand the target audience and their preferences. Research popular VR themes, genres, and settings to get an idea of what users would enjoy.</p></li>
<li><p><strong>Design New Environments</strong>: Use 3D modeling software like Blender or Unity to design new virtual environments for V-Sekai. Consider creating diverse worlds like fantasy realms, futuristic cities, natural landscapes, or historical settings.</p></li>
<li><p><strong>Implement Interactive Elements</strong>: Make the new worlds interactive by adding objects, characters, and elements that users can engage with. This can include non-playable characters (NPCs), puzzles, hidden items, or mini-games within each world.</p></li>
<li><p><strong>Test and Refine</strong>: Conduct thorough testing to ensure the new worlds are visually appealing, optimized for performance, and provide a seamless VR experience. Gather user feedback and make necessary improvements based on their suggestions.</p></li>
<li><p><strong>Integrate with V-Sekai</strong>: Once the new worlds are ready, integrate them into the V-Sekai platform. Ensure they are easily accessible through the menus or navigation system within V-Sekai’s VR interface.</p></li>
<li><p><strong>Release and Promote</strong>: Launch the updated version of V-Sekai with the newly generated worlds. Promote the release through various channels, such as social media, gaming forums, Virtual Reality communities, and V-Sekai’s official website.</p></li>
</ol>
<p>By following this strategy, we will not only address the current limitations but also create an engaging and immersive VR experience for V-Sekai users.</p>
</section>
<section id="upsides" class="level2">
<h2 class="anchored" data-anchor-id="upsides">Upsides</h2>
<ul>
<li>Increased user engagement: The availability of more worlds will provide users with greater options to explore, leading to longer sessions and increased immersion in V-Sekai.</li>
<li>Expanded user base: With a wider range of experiences, V-Sekai will attract new users who are looking for diverse VR environments.</li>
<li>Positive word-of-mouth: Satisfied users will share their experiences with others, generating positive reviews and recommendations for V-Sekai.</li>
</ul>
</section>
<section id="downsides" class="level2">
<h2 class="anchored" data-anchor-id="downsides">Downsides</h2>
<ul>
<li>Development complexity: Creating new worlds requires significant effort and resources, including 3D modeling, programming, and testing. This may lead to increased development time and costs.</li>
<li>Upfront investment: Generating more worlds will require upfront investment in terms of design, implementation, and integration with V-Sekai.</li>
<li>User feedback and iteration: Continuous user feedback is crucial to refine and improve the new worlds. This iterative process may extend the development timeline.</li>
</ul>
</section>
<section id="the-road-not-taken" class="level2">
<h2 class="anchored" data-anchor-id="the-road-not-taken">The Road Not Taken</h2>
<p>An alternative solution that was considered was <strong>Integrating Backend Services</strong>. This would involve improving the backend infrastructure of V-Sekai to enhance scalability, performance, and reliability. However, after careful consideration, we prioritize generating more worlds as it directly addresses user experience limitations and provides tangible benefits in terms of user engagement and appeal.</p>
</section>
<section id="in-core-and-done-by-us" class="level2">
<h2 class="anchored" data-anchor-id="in-core-and-done-by-us">In Core and Done by Us?</h2>
<p>The generation of more worlds is within the core capabilities of our team at V-Sekai. We have experienced 3D designers, developers, and testers who are skilled in creating immersive VR environments. Therefore, we are confident in our ability to execute this proposed solution effectively and efficiently.</p>
</section>
<section id="further-reading" class="level2">
<h2 class="anchored" data-anchor-id="further-reading">Further Reading</h2>
<ul>
<li><a href="https://v-sekai.org/">V-Sekai</a></li>
<li>This article is assisted by AI.</li>
</ul>
<p>If you have any further questions or need clarification on any aspect of this proposal, please feel free to ask.</p>


</section>
</section>

 ]]></description>
  <guid>https://v-sekai.github.io/manuals/decisions/20230830-generating-v-sekai-worlds.html</guid>
  <pubDate>Mon, 23 Oct 2023 22:52:52 GMT</pubDate>
</item>
<item>
  <title></title>
  <link>https://v-sekai.github.io/manuals/decisions/20230830-better-object-pick.html</link>
  <description><![CDATA[ 



<section id="improving-vr-experience" class="level1">
<h1>Improving VR Experience</h1>
<section id="metadata" class="level2">
<h2 class="anchored" data-anchor-id="metadata">Metadata</h2>
<ul>
<li>Status: proposed</li>
<li>Deciders: V-Sekai</li>
<li>Tags: V-Sekai</li>
</ul>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>V-Sekai is a virtual reality (VR) platform that offers users the opportunity to immerse themselves in virtual worlds. However, there are certain limitations that hinder the overall user experience. This documentation aims to address these limitations by proposing specific areas for improvement within V-Sekai.</p>
</section>
<section id="the-challenge" class="level2">
<h2 class="anchored" data-anchor-id="the-challenge">The Challenge</h2>
<p>V-Sekai currently faces several limitations, including non-functional menus in VR mode and a limited number of available worlds. To overcome these challenges and enhance the user experience, we must prioritize and focus on one area for improvement.</p>
<section id="proposed-solutions" class="level3">
<h3 class="anchored" data-anchor-id="proposed-solutions">Proposed Solutions</h3>
<p><strong>Improving Object Pickup and Menus</strong>: Enhancing object pickup mechanics and menus can make interactions within V-Sekai more intuitive and user-friendly. This may include refining controls, feedback mechanisms for object pickup, and finding alternative methods to access menus while in VR mode.</p>
</section>
</section>
<section id="the-strategy" class="level2">
<h2 class="anchored" data-anchor-id="the-strategy">The Strategy</h2>
<p>To ensure efficient use of time and resources, it is advisable to choose one area to focus on and strive to complete it within a two-week timeframe. This focused approach allows for significant progress in improving V-Sekai.</p>
</section>
<section id="upsides" class="level2">
<h2 class="anchored" data-anchor-id="upsides">Upsides</h2>
<p>By addressing these limitations, V-Sekai can offer a more seamless and immersive VR experience. Improving object pickup and menus will enhance user interactions, making them more intuitive and user-friendly. This will ultimately result in a more engaging and enjoyable experience for V-Sekai users.</p>
</section>
<section id="downsides" class="level2">
<h2 class="anchored" data-anchor-id="downsides">Downsides</h2>
<p>Given the limited timeframe, it may not be possible to address all the identified limitations simultaneously. Additionally, the complexity of refining object pickup mechanics and menus may present challenges in achieving substantial improvements within the two-week timeframe. It’s important to carefully assess the scope and feasibility of the proposed enhancements before committing to the timeframe.</p>
</section>
<section id="the-road-not-taken" class="level2">
<h2 class="anchored" data-anchor-id="the-road-not-taken">The Road Not Taken</h2>
<p>While focusing on improving object pickup and menus is recommended for the given timeframe, it’s important to acknowledge that there are other potential areas for improvement. Exploring options such as expanding the variety of worlds, integrating backend services, or optimizing performance can be considered in future iterations of V-Sekai.</p>
</section>
<section id="in-core-and-done-by-us" class="level2">
<h2 class="anchored" data-anchor-id="in-core-and-done-by-us">In Core and Done by Us?</h2>
<p>The development team at V-Sekai should primarily handle the implementation and integration of the chosen improvement. However, collaborative efforts involving developers, designers, and user experience experts may be necessary to ensure the improvements align with the overall vision and goals of V-Sekai.</p>
</section>
<section id="further-reading" class="level2">
<h2 class="anchored" data-anchor-id="further-reading">Further Reading</h2>
<ul>
<li><a href="https://v-sekai.org/">V-Sekai</a></li>
<li>This article is assisted by AI.</li>
</ul>


</section>
</section>

 ]]></description>
  <guid>https://v-sekai.github.io/manuals/decisions/20230830-better-object-pick.html</guid>
  <pubDate>Mon, 23 Oct 2023 22:52:52 GMT</pubDate>
</item>
<item>
  <title></title>
  <link>https://v-sekai.github.io/manuals/decisions/20230830-better-backend-services.html</link>
  <description><![CDATA[ 



<section id="integrating-backend-services" class="level1">
<h1>Integrating Backend Services</h1>
<section id="metadata" class="level2">
<h2 class="anchored" data-anchor-id="metadata">Metadata</h2>
<ul>
<li>Status: proposed</li>
<li>Deciders: V-Sekai</li>
<li>Tags: V-Sekai</li>
</ul>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>V-Sekai is a virtual reality (VR) platform that offers users the opportunity to immerse themselves in virtual worlds. However, there are certain limitations that hinder the overall user experience. This documentation aims to address these limitations by proposing specific areas for improvement within V-Sekai.</p>
</section>
<section id="the-challenge" class="level2">
<h2 class="anchored" data-anchor-id="the-challenge">The Challenge</h2>
<p>V-Sekai currently faces several limitations, including non-functional menus in VR mode and a limited number of available worlds. To overcome these challenges and enhance the user experience, we must prioritize and focus on one area for improvement.</p>
<section id="proposed-solutions" class="level3">
<h3 class="anchored" data-anchor-id="proposed-solutions">Proposed Solutions</h3>
<p><strong>Exploring Backend Services Integration</strong>: Integrating V-Sekai with backend services can enhance the platform’s features and capabilities. This could involve incorporating external APIs or services to provide additional resources and functionalities.</p>
</section>
</section>
<section id="the-strategy" class="level2">
<h2 class="anchored" data-anchor-id="the-strategy">The Strategy</h2>
<p>To ensure efficient use of time and resources, it is advisable to choose one area to focus on and strive to complete it within a two-week timeframe. This focused approach allows for significant progress in improving V-Sekai.</p>
</section>
<section id="upsides" class="level2">
<h2 class="anchored" data-anchor-id="upsides">Upsides</h2>
<p>By addressing these limitations, V-Sekai can be transformed into a more immersive and engaging VR platform. Enhancing menus and object pickup mechanics, expanding the variety of worlds, or integrating backend services can greatly enhance the overall user experience for users.</p>
</section>
<section id="downsides" class="level2">
<h2 class="anchored" data-anchor-id="downsides">Downsides</h2>
<p>Given the limited timeframe, it may not be possible to address all the identified limitations simultaneously. Prioritizing one area means that other aspects may need to wait for future improvements. Additionally, the complexity of the chosen area may present challenges in achieving substantial improvements within the two-week timeframe.</p>
</section>
<section id="the-road-not-taken" class="level2">
<h2 class="anchored" data-anchor-id="the-road-not-taken">The Road Not Taken</h2>
<p>While focusing on one area is recommended for the given timeframe, it’s important to acknowledge that there are other potential avenues for improvement. Exploring options such as performance optimization, improving UI design, or implementing additional features can be considered in future iterations of V-Sekai.</p>
</section>
<section id="in-core-and-done-by-us" class="level2">
<h2 class="anchored" data-anchor-id="in-core-and-done-by-us">In Core and Done by Us?</h2>
<p>The development team at V-Sekai should primarily handle the implementation and integration of the chosen improvement. However, collaborative efforts, involving developers, designers, and technical writers, are essential for documenting the enhancements.</p>
</section>
<section id="further-reading" class="level2">
<h2 class="anchored" data-anchor-id="further-reading">Further Reading</h2>
<ul>
<li><a href="https://v-sekai.org/">V-Sekai</a></li>
<li>This article is assisted by AI.</li>
</ul>


</section>
</section>

 ]]></description>
  <guid>https://v-sekai.github.io/manuals/decisions/20230830-better-backend-services.html</guid>
  <pubDate>Mon, 23 Oct 2023 22:52:52 GMT</pubDate>
</item>
<item>
  <title></title>
  <link>https://v-sekai.github.io/manuals/decisions/20230829-omifest-2023.html</link>
  <description><![CDATA[ 



<section id="omifest-2023---the-call" class="level1">
<h1>OMIFest 2023 - The Call</h1>
<section id="metadata" class="level2">
<h2 class="anchored" data-anchor-id="metadata">Metadata</h2>
<ul>
<li><strong>Status</strong>: Proposed</li>
<li><strong>Deciders</strong>: OMIGroup, Fire, Indiebio</li>
<li><strong>Tags</strong>: OMIGroup, V-Sekai</li>
</ul>
</section>
<section id="context" class="level2">
<h2 class="anchored" data-anchor-id="context">Context</h2>
<section id="the-backdrop" class="level3">
<h3 class="anchored" data-anchor-id="the-backdrop">The Backdrop</h3>
<p>OMIFest 2023 is an online event set against the backdrop of a rapidly evolving digital landscape. As virtual reality and augmented reality technologies become more accessible, there’s an increasing need for platforms that allow artists, tinkerers, hackers, players, and professionals to showcase their talents and collaborate in a digital environment.</p>
</section>
<section id="the-challenge" class="level3">
<h3 class="anchored" data-anchor-id="the-challenge">The Challenge</h3>
<p>The challenge is to create an event that not only celebrates these individuals but also encourages conversation and collaboration within our community. We aim to build a webpage on the OMI website for more information and updates about this online event.</p>
</section>
</section>
<section id="decision" class="level2">
<h2 class="anchored" data-anchor-id="decision">Decision</h2>
<section id="the-strategy" class="level3">
<h3 class="anchored" data-anchor-id="the-strategy">The Strategy</h3>
<p>Our strategy is to host a series of monthly events leading up to the main event in October 2024. Each month we will focus on different aspects of the event, such as building the vehicle, avatars, textures, scenery, and workshops on content creation.</p>
<section id="goals-sorted-by-ease" class="level4">
<h4 class="anchored" data-anchor-id="goals-sorted-by-ease">Goals (Sorted by ease)</h4>
<ol type="1">
<li><strong>Research</strong>: Held in conjunction with an academically minded in-person conference.</li>
<li><strong>Assets, protocols, and specification testing</strong>: Build assets and test protocols to ensure interoperability.</li>
<li><strong>Community Celebration</strong>: Celebrate with the global community in a digital setting.</li>
</ol>
</section>
<section id="who-can-participate" class="level4">
<h4 class="anchored" data-anchor-id="who-can-participate">Who Can Participate?</h4>
<p>This online event is open to everyone. Individuals, teams, corporate members, community groups, anyone and everyone can submit an entry. Registration will be required for our information, and some moderation may be required, but everyone is welcome.</p>
</section>
<section id="event-features-sorted-by-ease" class="level4">
<h4 class="anchored" data-anchor-id="event-features-sorted-by-ease">Event Features (Sorted by ease)</h4>
<ol type="1">
<li>Notice boards for help needed, job board, find a team etc</li>
<li>Limited poster advertising</li>
<li>Music</li>
<li>Booths for nonprofit, education, community organisations</li>
<li>Metaverse library/blockbuster to find how-to guides, videos, etc</li>
<li>Thrift shop for free stuff</li>
<li>Market for assets</li>
<li>Junk yard for abandoned assets</li>
<li>Portals to other worlds</li>
<li>Assets that are created remain in the world</li>
<li>Scenery around the parade</li>
<li>Parade with floats</li>
</ol>
</section>
<section id="cost-to-enter" class="level4">
<h4 class="anchored" data-anchor-id="cost-to-enter">Cost to Enter</h4>
<p>It is free to register and submit assets. The event will be free to attend. Some aspects and mini events may require payment.</p>
</section>
<section id="sponsorship" class="level4">
<h4 class="anchored" data-anchor-id="sponsorship">Sponsorship</h4>
<p>We are open to sponsorships for this online event. More details will be provided soon.</p>
</section>
<section id="calendar-of-events" class="level4">
<h4 class="anchored" data-anchor-id="calendar-of-events">Calendar of Events</h4>
<p>Here is a rough schedule of our monthly events:</p>
<table class="table">
<thead>
<tr class="header">
<th>Date</th>
<th>Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>August 2023</td>
<td>Content sharing workshop</td>
</tr>
<tr class="even">
<td>September 2023</td>
<td>Making a basic vehicle</td>
</tr>
<tr class="odd">
<td>October 2023</td>
<td>Making an avatar</td>
</tr>
<tr class="even">
<td>November 2023</td>
<td>Making textures</td>
</tr>
<tr class="odd">
<td>December 2023</td>
<td>Making scenery</td>
</tr>
<tr class="even">
<td>January 2024</td>
<td>Music in the metaverse</td>
</tr>
<tr class="odd">
<td>February 2024</td>
<td>AI</td>
</tr>
<tr class="even">
<td>March 2024</td>
<td>Community, public spheres</td>
</tr>
<tr class="odd">
<td>April 2024</td>
<td>Portals</td>
</tr>
<tr class="even">
<td>May 2024</td>
<td>(repeat the above?)</td>
</tr>
<tr class="odd">
<td>June 2024</td>
<td>Use art to explore data</td>
</tr>
<tr class="even">
<td>July 2024</td>
<td>Real world challenge</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="the-upside" class="level3">
<h3 class="anchored" data-anchor-id="the-upside">The Upside</h3>
<p>The upside of this online event is that it provides a platform for individuals from various backgrounds to showcase their talents, learn from each other, and collaborate on projects. It also helps in promoting the use of VR/AR technologies and encourages innovation in this field.</p>
</section>
<section id="the-downside" class="level3">
<h3 class="anchored" data-anchor-id="the-downside">The Downside</h3>
<p>The downside could be the potential logistical challenges involved in organizing such a large-scale online event. There might also be issues related to ensuring fair participation and dealing with intellectual property rights.</p>
</section>
<section id="the-road-not-taken" class="level3">
<h3 class="anchored" data-anchor-id="the-road-not-taken">The Road Not Taken</h3>
<p>An alternative approach could have been to organize smaller, more frequent online events throughout the year. However, we decided to go with a larger annual event to create more impact and allow participants enough time to prepare.</p>
</section>
<section id="the-infrequent-use-case" class="level3">
<h3 class="anchored" data-anchor-id="the-infrequent-use-case">The Infrequent Use Case</h3>
<p>While the event is designed to cater to a wide range of participants, there might be some who might not be able to participate due to lack of access to necessary technology or skills. We plan to address this by providing resources and learning materials to help them get started.</p>
</section>
<section id="in-core-and-done-by-us" class="level3">
<h3 class="anchored" data-anchor-id="in-core-and-done-by-us">In Core and Done by Us?</h3>
<p>Yes, the event is being organized by us, OMIGroup, in collaboration with V-Sekai. We are responsible for planning, coordinating, and executing aspects of the event. The success of such a large-scale online event relies on the collective efforts of many individuals and organizations.</p>
</section>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><a href="https://omigroup.org/">OMIGroup</a></li>
<li><a href="https://v-sekai.org/">V-Sekai</a></li>
<li><a href="https://hackmd.io/@indiebio/metaverse-float-parade">Metaverse Float Parade</a></li>
<li>This article is assisted by AI.</li>
</ul>


</section>
</section>

 ]]></description>
  <guid>https://v-sekai.github.io/manuals/decisions/20230829-omifest-2023.html</guid>
  <pubDate>Mon, 23 Oct 2023 22:52:52 GMT</pubDate>
</item>
<item>
  <title></title>
  <link>https://v-sekai.github.io/manuals/decisions/20230824-subdiv-for-lods.html</link>
  <description><![CDATA[ 



<section id="use-opensubdiv-for-lods" class="level1">
<h1>Use opensubdiv for lods</h1>
<section id="metadata" class="level2">
<h2 class="anchored" data-anchor-id="metadata">Metadata</h2>
<ul>
<li>Status: proposed <!-- draft | proposed | rejected | accepted | deprecated | superseded by --><br>
</li>
<li>Deciders: V-Sekai, fire</li>
<li>Tags: V-Sekai</li>
</ul>
</section>
<section id="the-backdrop" class="level2">
<h2 class="anchored" data-anchor-id="the-backdrop">The Backdrop</h2>
<p>Creators want more. Players want more. The platform can only support a limited amount of content.</p>
</section>
<section id="the-challenge" class="level2">
<h2 class="anchored" data-anchor-id="the-challenge">The Challenge</h2>
<p>The challenge lies in optimizing the content to fit within the platform’s limitations without compromising on the quality and detail.</p>
</section>
<section id="the-strategy" class="level2">
<h2 class="anchored" data-anchor-id="the-strategy">The Strategy</h2>
<p>Use OpenSubdiv to increase detail, which can then be reduced through Godot Engine’s LOD (Level of Detail) scheme. This allows for high-quality models that are optimized for performance.</p>
</section>
<section id="the-upside" class="level2">
<h2 class="anchored" data-anchor-id="the-upside">The Upside</h2>
<p>The upside is that this approach allows for more detailed models while still maintaining good performance.</p>
</section>
<section id="the-downside" class="level2">
<h2 class="anchored" data-anchor-id="the-downside">The Downside</h2>
<p>The downside is that it requires additional processing to create the LOD models, which could potentially slow down the development process.</p>
</section>
<section id="the-road-not-taken" class="level2">
<h2 class="anchored" data-anchor-id="the-road-not-taken">The Road Not Taken</h2>
<p>An alternative approach would have been to manually create lower detail models for use in less demanding scenarios. However, this would require significantly more work from the creators.</p>
</section>
<section id="the-infrequent-use-case" class="level2">
<h2 class="anchored" data-anchor-id="the-infrequent-use-case">The Infrequent Use Case</h2>
<p>This strategy might not be beneficial for very simple models or scenes where the level of detail is not a significant factor.</p>
</section>
<section id="in-core-and-done-by-us" class="level2">
<h2 class="anchored" data-anchor-id="in-core-and-done-by-us">In Core and Done by Us?</h2>
<p>Yes, this strategy is implemented in the core of our platform and maintained by us.</p>
</section>
<section id="further-reading" class="level2">
<h2 class="anchored" data-anchor-id="further-reading">Further Reading</h2>
<ul>
<li><a href="https://v-sekai.org/">V-Sekai</a><br>
</li>
<li>This article is assisted by AI.</li>
</ul>


</section>
</section>

 ]]></description>
  <guid>https://v-sekai.github.io/manuals/decisions/20230824-subdiv-for-lods.html</guid>
  <pubDate>Mon, 23 Oct 2023 22:52:52 GMT</pubDate>
</item>
<item>
  <title></title>
  <link>https://v-sekai.github.io/manuals/decisions/20230823-speech-to-blendshapes.html</link>
  <description><![CDATA[ 



<section id="developing-a-unified-model-for-speech-to-facial-shape-generation-using-outlines-library" class="level1">
<h1>Developing a Unified Model for Speech-to-Facial Shape Generation Using Outlines 〰 Library</h1>
<section id="metadata" class="level2">
<h2 class="anchored" data-anchor-id="metadata">Metadata</h2>
<ul>
<li>Status: proposed</li>
<li>Deciders: V-Sekai</li>
<li>Tags: V-Sekai</li>
</ul>
</section>
<section id="the-backdrop" class="level2">
<h2 class="anchored" data-anchor-id="the-backdrop">The Backdrop</h2>
<p>V-Sekai is embarking on a project that translates speech into corresponding facial shapes for virtual reality applications. This process eliminates the need for a face camera and leverages the <code>SpeechEncoderDecoderModel</code> from Hugging Face’s Transformers library for comprehensive speech recognition.</p>
<p>The SpeakingFaces dataset, a collection from 142 diverse subjects, will be utilized for this project. The dataset is accessible via the server of the Institute for Smart Systems and Artificial Intelligence (ISSAI) under the Creative Commons Attribution 4.0 International License.</p>
<p>Outlines 〰, a neural text generation library, will also play a crucial role in this project. It offers robust prompting primitives, separates the prompting from the execution logic, and is compatible with all models. It interfaces with models through the next-token logits and can be used with API-based models as well.</p>
</section>
<section id="the-challenge" class="level2">
<h2 class="anchored" data-anchor-id="the-challenge">The Challenge</h2>
<p>The primary challenge lies in developing a model capable of generating text and blendshapes simultaneously from the same input speech using the Outlines 〰 library.</p>
</section>
<section id="the-strategy" class="level2">
<h2 class="anchored" data-anchor-id="the-strategy">The Strategy</h2>
<p>The proposed strategy involves creating a custom model that accepts speech as input and outputs both text and blendshapes. This unified model would incorporate a shared encoder to process the speech input, and a single decoder to generate the text and blendshape outputs. The Outlines 〰 library will be used to conform the output to the desired format.</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> outlines <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Outline</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> transformers <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> SpeechEncoderDecoderModel, Speech2Text2Processor, Wav2Vec2Model</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> json</span>
<span id="cb1-4"></span>
<span id="cb1-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Initialize the processor and encoder</span></span>
<span id="cb1-6">processor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Speech2Text2Processor.from_pretrained(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"facebook/s2t-small-librispeech-asr"</span>)</span>
<span id="cb1-7">encoder <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Wav2Vec2Model.from_pretrained(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"facebook/wav2vec2-base-960h"</span>)</span>
<span id="cb1-8"></span>
<span id="cb1-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Initialize the decoder</span></span>
<span id="cb1-10">decoder <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Speech2Text2Model.from_pretrained(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"facebook/s2t-small-librispeech-asr"</span>)</span>
<span id="cb1-11"></span>
<span id="cb1-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Initialize the unified model</span></span>
<span id="cb1-13">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> UnifiedSpeechEncoderDecoderModel(encoder<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>encoder, decoder<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>decoder)</span>
<span id="cb1-14"></span>
<span id="cb1-15"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Define an outline for the model</span></span>
<span id="cb1-16">outline <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Outline(model).to(device)</span>
<span id="cb1-17"></span>
<span id="cb1-18"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Generate text and blendshapes</span></span>
<span id="cb1-19">generated_text, generated_blendshapes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> outline.generate(input_speech)</span>
<span id="cb1-20"></span>
<span id="cb1-21"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Conform to JSON schemaßß</span></span>
<span id="cb1-22">blendshapes_keys <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"BrowInnerUp"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"BrowDownLeft"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"BrowDownRight"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"BrowOuterUpLeft"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"BrowOuterUpRight"</span>,</span>
<span id="cb1-23">                    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"EyeLookUpLeft"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"EyeLookUpRight"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"EyeLookDownLeft"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"EyeLookDownRight"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"EyeLookInLeft"</span>,</span>
<span id="cb1-24">                    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"EyeLookInRight"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"EyeLookOutLeft"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"EyeLookOutRight"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"EyeBlinkLeft"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"EyeBlinkRight"</span>,</span>
<span id="cb1-25">                    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"EyeSquintRight"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"EyeSquintLeft"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"EyeWideLeft"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"EyeWideRight"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"CheekPuff"</span>,</span>
<span id="cb1-26">                    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"CheekSquintLeft"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"CheekSquintRight"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"NoseSneerLeft"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"NoseSneerRight"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"JawOpen"</span>,</span>
<span id="cb1-27">                    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"JawForward"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"JawLeft"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"JawRight"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MouthFunnel"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MouthPucker"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MouthLeft"</span>,</span>
<span id="cb1-28">                    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MouthRight"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MouthRollUpper"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MouthRollLower"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MouthShrugUpper"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MouthShrugLower"</span>,</span>
<span id="cb1-29">                    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MouthClose"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MouthSmileLeft"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MouthSmileRight"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MouthFrownLeft"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MouthFrownRight"</span>,</span>
<span id="cb1-30">                    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MouthDimpleLeft"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MouthDimpleRight"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MouthUpperUpLeft"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MouthUpperUpRight"</span>,</span>
<span id="cb1-31">                    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MouthLowerDownLeft"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MouthLowerDownRight"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MouthPressLeft"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MouthPressRight"</span>,</span>
<span id="cb1-32">                    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MouthStretchLeft"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MouthStretchRight"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"TongueOut"</span>]</span>
<span id="cb1-33"></span>
<span id="cb1-34">output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb1-35">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"text"</span>: generated_text,</span>
<span id="cb1-36">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"blendshapes"</span>: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">zip</span>(blendshapes_keys, generated_blendshapes))</span>
<span id="cb1-37">}</span>
<span id="cb1-38"></span>
<span id="cb1-39">json_output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> json.dumps(output)</span></code></pre></div>
</section>
<section id="the-upside" class="level2">
<h2 class="anchored" data-anchor-id="the-upside">The Upside</h2>
<p>Developing our own unified model for generating both text and blendshapes from speech using the Outlines 〰 library is a strategic move that provides us with greater control and customization to meet our specific needs. It not only deepens our understanding of the model, aiding in troubleshooting and improvements, but also allows us to optimize it for better performance and efficiency.</p>
</section>
<section id="the-downside" class="level2">
<h2 class="anchored" data-anchor-id="the-downside">The Downside</h2>
<p>The limitation of this approach is that the <code>UnifiedSpeechEncoderDecoderModel</code>, and <code>Outline</code> are placeholders for your actual unified model classes, and outline respectively. You would need to implement these classes yourself, or find pre-trained models that suit your needs.</p>
</section>
<section id="the-road-not-taken" class="level2">
<h2 class="anchored" data-anchor-id="the-road-not-taken">The Road Not Taken</h2>
<p>Being self-reliant in maintaining and updating our model eliminates dependence on third parties, providing flexibility to make changes as needed. Moreover, it ensures seamless integration with other parts of our system.</p>
</section>
<section id="the-infrequent-use-case" class="level2">
<h2 class="anchored" data-anchor-id="the-infrequent-use-case">The Infrequent Use Case</h2>
<p>While this approach requires more initial effort, the potential for improvement and fine-tuning with our unique data sets makes it worthwhile, leading to superior results in the long run.</p>
</section>
<section id="in-core-and-done-by-us" class="level2">
<h2 class="anchored" data-anchor-id="in-core-and-done-by-us">In Core and Done by Us?</h2>
<p>Yes, the development and maintenance of the unified model will be done by us.</p>
</section>
<section id="further-reading" class="level2">
<h2 class="anchored" data-anchor-id="further-reading">Further Reading</h2>
<ul>
<li><a href="https://v-sekai.org/">V-Sekai</a></li>
<li>This article is assisted by AI.</li>
</ul>


</section>
</section>

 ]]></description>
  <guid>https://v-sekai.github.io/manuals/decisions/20230823-speech-to-blendshapes.html</guid>
  <pubDate>Mon, 23 Oct 2023 22:52:52 GMT</pubDate>
</item>
</channel>
</rss>
