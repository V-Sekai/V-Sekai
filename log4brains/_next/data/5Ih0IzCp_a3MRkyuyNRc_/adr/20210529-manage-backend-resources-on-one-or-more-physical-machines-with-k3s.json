{"pageProps":{"projectName":"V-Sekai","currentAdr":{"slug":"20210529-manage-backend-resources-on-one-or-more-physical-machines-with-k3s","package":null,"title":"Manage backend resources on one or more physical machines with K3S","status":"accepted","supersededBy":null,"tags":[],"deciders":[],"body":{"enhancedMdx":"\n## Context and Problem Statement\n\nWe want a way to manage backend resources on one or more physical machines.\n\n## Describe the proposed option and how it helps to overcome the problem or limitation\n\nWe are using a lightweight Kubernetes implementation, k3s, to provide management of backend resources\n\n## Describe how your proposal will work, with code, pseudo-code, mock-ups, or diagrams\n\n### System Setup (Done once)\n\nInstall Fedora 34 with a user account set as administrator.\n\nAdd authorized keys into ~/.ssh/authorized_keys\n\nOPTIONAL: customize SSH port:\nSet `Port 2345` in /etc/ssh/sshd_config.\n```bash\nsudo semanage port -a -t ssh_port_t -p tcp 2345\nsudo firewall-cmd --add-port=2345/tcp\n```\n\nREQUIRED: set `PasswordAuthentication no` in /etc/ssh/sshd_config.\n\nOPTIONAL: Configure default DNS server:\n\n\n#### /etc/NetworkManager/conf.d/override-dns.conf\n\n```ini\n[main]\ndns=none\n[ipv4]\nmethod=auto\ndns=8.8.8.8;4.2.2.2;\nignore-auto-dns=true\n```\n\n#### /etc/resolv.conf\n\n```\nsearch localhost\nnameserver 1.0.0.1\nnameserver 1.1.1.1\n```\n\n#### sshd restart\n\n```bash\nsudo service sshd restart\nsudo firewall-cmd --add-masquerade --permanent\nsudo systemctl restart firewalld\nsudo yum install redhat-lsb-core container-selinux selinux-policy-base podman\nsudo rpm -i https://github.com/rancher/k3s-selinux/releases/download/v0.1.1-rc1/k3s-selinux-0.1.1-rc1.el7.noarch.rpm\n# Validate that ssh logins work, then run:\nsudo service NetworkManager stop; sudo service NetworkManager start\nsudo grubby --update-kernel=ALL --args=\"systemd.unified_cgroup_hierarchy=0\"\n```\nNow, reboot the system.\n\nFirst, create a new LVM Logical Volume named kubepvc XFS 100GB mounted at `/kube`.\nAdd the following to /etc/fstab:\n\n#### rancher\n\n```\n# If created manually, run `sudo blkid` and add:\n# UUID=aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee /kube   auto  defaults   0 0\n/kube/rancher /var/lib/rancher    none bind 0 0`\n```\n#### kube partition\n\nNow, setup the partition:\n```bash\nsudo mkdir -p /kube\nsudo mount /kube\nsudo mkdir -p /kube/pvc /kube/rancher /var/lib/rancher /root/.kube\nsudo chcon -R -t container_file_t /kube/pvc\nsudo mount /var/lib/rancher\n```\n\n### SELinux patches\n\nIf your system uses SELinux, add the following SELinux module to allow dev-shm access. This will be necessary for python multiprocessing in containers, specifically usage of shm_lock, which is used by Emscripten.\n\n#### dev-shm.te\n\n```\nmodule dev-shm 1.0;\n\nrequire {\n        type container_runtime_tmpfs_t;\n        type container_t;\n        class dir { getattr relabelto unlink ioctl execute append read setattr write lock create rename link search rmdir remove_name reparent add_name open };\n        class file { map getattr relabelto unlink ioctl execute append read setattr write lock create rename relabelfrom link execute_no_trans entrypoint execmod open };\n        class shm { write destroy unix_write getattr create read setattr unix_read associate lock };\n        class sock_file { getattr relabelto unlink ioctl execute append read setattr write lock create rename relabelfrom link open };\n        class fifo_file { getattr relabelto unlink ioctl execute append read setattr write lock create rename relabelfrom link open };\n        class lnk_file { getattr relabelto unlink ioctl execute append read setattr write lock create rename relabelfrom link };\n}\n#============= container_t ==============\nallow container_t container_runtime_tmpfs_t:dir { getattr relabelto unlink ioctl execute append read setattr write lock create rename link search rmdir remove_name reparent add_name open };\n# domain_can_mmap_files\nallow container_t container_runtime_tmpfs_t:file { map getattr relabelto unlink ioctl execute append read setattr write lock create rename relabelfrom link execute_no_trans entrypoint execmod open };\nallow container_t container_runtime_tmpfs_t:shm { write destroy unix_write getattr create read setattr unix_read associate lock };\nallow container_t container_runtime_tmpfs_t:sock_file { getattr relabelto unlink ioctl execute append read setattr write lock create rename relabelfrom link open };\nallow container_t container_runtime_tmpfs_t:fifo_file { getattr relabelto unlink ioctl execute append read setattr write lock create rename relabelfrom link open };\nallow container_t container_runtime_tmpfs_t:lnk_file { getattr relabelto unlink ioctl execute append read setattr write lock create rename relabelfrom link };\n```\n\nNow, install it with:\n```bash\ncheckmodule -M -m dev-shm.te -o dev-shm.mod\nsemodule_package -m dev-shm.mod -o dev-shm.pp\nsudo semodule -i dev-shm.pp\n```\n\n### Install k3s (Start here if recreating)\n\n```bash\ncurl -sfL https://raw.githubusercontent.com/rancher/k3s/master/install.sh | sh -s - server -o /root/.kube/config --default-local-storage-path /kube/pvc --no-deploy=servicelb --disable=traefik --disable=servicelb\n```\n\nNow you will need to obtain the DNS IAM credentials. In this example, we are working with these Route53 hosted domains: vsekai.org,vsekai.net,vsekai.com,vsekai.cloud,v-sekai.org,v-sekai.net,v-sekai.com,v-sekai.cloud,vsek.ai,v-sek.ai\n\nInstall Helm services:\n\n```bash\nACCESSKEY=AKIAABCDEFGHIJKLMNOP\nSECRETKEY=XXXXXXXXXXXXXXXXXXXX\n\nsudo helm repo add gocd https://gocd.github.io/helm-chart\nsudo helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nsudo helm repo add bitnami https://charts.bitnami.com/bitnami\nsudo helm repo add jetstack https://charts.jetstack.io\nsudo helm repo update\n\n# We are still using 3.2.3 - I have not figured out the upgrade process yet for external-dns\n\nsudo helm install external-dns --set provider=aws --set aws.zoneType=public --set registry=noop --set aws.credentials.accessKey=\"$ACCESSKEY\" --set domainFilters='{vsekai.org,vsekai.net,vsekai.com,vsekai.cloud,v-sekai.org,v-sekai.net,v-sekai.com,v-sekai.cloud,vsek.ai,v-sek.ai}' --set aws.credentials.secretKey=\"$SECRETKEY\"  bitnami/external-dns\n\nsudo kubectl create namespace ingress-nginx\nsudo helm install --namespace ingress-nginx nginx ingress-nginx/ingress-nginx --set controller.replicaCount=2\nsudo kubectl patch svc/nginx-nginx-ingress-controller -n kube-system --patch '{\"spec\":{\"externalTrafficPolicy\":\"Local\"}}'\nsudo kubectl patch deployments/nginx-nginx-ingress-controller --patch '{\"spec\":{\"template\":{\"spec\":{\"hostNetwork\":true}}}}' -n kube-system\n\nsudo kubectl get replicasets -n kube-system\n# Find the oldest nginx-nginx-ingress-controller one and delete with\nsudo kubectl delete replicasets -nginx-ingress-controller-abcdefg-whatever -n kube-system\n\nsudo kubectl create namespace cert-manager\n# Used --version v0.15.1 before; now v1.3.1\nsudo helm install cert-manager jetstack/cert-manager --namespace cert-manager --version v1.3.1 --set installCRDs=true\nsudo kubectl --namespace cert-manager create secret generic prod-route53-credentials-secret --from-literal=secret-access-key=\"$SECRETKEY\"\n```\n\n#### ingress.yaml (edit accessKeyID)\n\n```yaml\napiVersion: cert-manager.io/v1alpha2\nkind: ClusterIssuer\nmetadata:\n name: letsencrypt-prod\n namespace: cert-manager\nspec:\n acme:\n   # The ACME server URL\n   server: https://acme-v02.api.letsencrypt.org/directory\n   # Email address used for ACME registration\n   email: example@example.com\n   # Name of a secret used to store the ACME account private key\n   privateKeySecretRef:\n     name: letsencrypt-prod\n   # Enable the HTTP-01 challenge provider\n   solvers:\n   - http01:\n       ingress:\n         class: nginx\n   #solvers:\n   #- dns01:\n   #   route53:\n   #     region: us-east-1\n   #     accessKeyID: AKIAABCDEFGHIJKLMNOP\n   #     secretAccessKeySecretRef:\n   #       name: prod-route53-credentials-secret\n   #       key: secret-access-key\n---\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: core-ingress\n  annotations:\n    kubernetes.io/ingress.class: \"nginx\"\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n    nginx.ingress.kubernetes.io/hsts: \"false\"\n    # nginx.ingress.kubernetes.io/add-base-url: \"true\"\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/configuration-snippet: |\n      if ($host ~ ^(?!(ci))) {\n        more_clear_headers \"Strict-Transport-Security\";\n      }\n      if ($host ~ ^(.*\\.?)vsekai\\.([a-z]*)$ ) {\n          set $subd $1;\n          set $tld $2;\n          set $newdomain v-sekai.$tld;\n          rewrite ^/(.*)$ https://$subd$newdomain/$1;\n      }\nspec:\n  tls:\n  - hosts:\n    - vsekai.cloud\n    - v-sekai.cloud\n    - uro.vsekai.cloud\n    - uro.v-sekai.cloud\n    - radiance.v-sekai.cloud\n    - ci.radiance.v-sekai.cloud\n    - ci.v-sekai.cloud\n    - radiance.vsekai.cloud\n    - ci.radiance.vsekai.cloud\n    - ci.vsekai.cloud\n    - hls.vsek.ai\n    secretName: radiance-cert-secret1\n  rules:\n  - host: vsekai.cloud\n  - host: v-sekai.cloud\n  - host: uro.vsekai.cloud\n  - host: uro.v-sekai.cloud\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: uro\n          servicePort: 4000\n  - host: hls.vsek.ai\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: nginx-rtmp-service\n          servicePort: 80\n  - host: radiance.vsekai.cloud\n  - host: radiance.v-sekai.cloud\n    http:\n      paths:\n      - path: /debug_headers\n        backend:\n          serviceName: echo-service\n          servicePort: 8099\n      - path: /\n        backend:\n          serviceName: echo1\n          servicePort: 80\n  - host: ci.vsekai.cloud\n  - host: ci.v-sekai.cloud\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: gocd-server\n          servicePort: 8153\n  - host: ci.radiance.vsekai.cloud\n  - host: ci.radiance.v-sekai.cloud\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: gocd-server\n          servicePort: 8153\n```\nThen:\n```bash\nsudo kubectl apply -f ingress.yaml\n```\n\n### Setting up metallb:\n\n#### metalconfig.yaml\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: metallb-system\n  name: config\ndata:\n  config: |\n    address-pools:\n    - name: default\n      protocol: layer2\n      addresses:\n      - 108.85.109.49/32 # <--- this is the external. HOST IP\n```\n\n```bash\nsudo kubectl create namespace metallb-system\nsudo kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey=\"$(openssl rand -base64 128)\"\nsudo kubectl apply -f metalconfig.yml\nsudo kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/metallb.yaml\n```\n\n### Setting up osxcross\n\n(Optional in theory, but you have to remove enabled platforms in gocd):\n\nStart by grabbing a legitmate copy of `MacOSX10.15.sdk.tar.xz` from your local MacBook or Mac Mini.\n\nThen do the following to build:\n\n```bash\ncd /opt\nsudo chown `whoami` .\ngit clone https://github.com/tpoechtrager/osxcross.git /opt/osxcross\nsudo chown root .\ncd /opt/osxcross\nmv MacOSX10.15.sdk.tar.xz /opt/osxcross/tarballs\nUNATTENDED=1 ./build.sh\n\n# In case host OS is newer than guest OS:\ncp -p /lib64/libstdc++.so.6 /opt/osxcross/target/bin/\n```\n\nOn SELinux systems, you may need to give containers read permissions:\n```bash\nsudo chcon -R unconfined_u:object_r:container_share_t:s0  /opt/osxcross\n```\n\n### Setting up gocd:\n\n#### gocd_values.yaml\n\n```yaml\nagent:\n  image:\n    # agent.image.repository is the GoCD Agent image name\n    repository: \"groupsinfra/gocd-agent-centos-8-groups\" # agent.image.tag is the GoCD Agent image's tag\n    tag: v20.7.0-groups-0.5.3\n    # agent.image.pullPolicy is the GoCD Agent image's pull policy\n\n    pullPolicy: \"IfNotPresent\" # agent.replicaCount is the GoCD Agent replicas Count. Specify the number of GoCD agents to run\n  replicaCount: 6\n  security:\n    ssh:\n      enabled: true\n      secretName: gocd-ssh\nserver:\n  shouldPreconfigure: false\n  security:\n    ssh:\n      enabled: true\n      secretName: gocd-ssh\n  env:\n    extraEnvVars:\n      - name: GOCD_PLUGIN_INSTALL_gitlab-oauth-authorization-plugin\n        value: https://github.com/gocd-contrib/gitlab-oauth-authorization-plugin/releases/download/v2.0.1-52-exp/gitlab-oauth-authorization-plugin-2.0.1-52.jar\n```\n\n##### Keygen\n\n```bash\nssh-keygen -t rsa -b 4096 -C \"gocd-ssh-key\" -f gocd-ssh -P ''\n( ssh-keyscan gitlab.com ; ssh-keyscan github.com ) > gocd_known_hosts\nsudo kubectl create secret generic gocd-ssh --from-file=id_rsa=gocd-ssh --from-file=id_rsa.pub=gocd-ssh.pub --from-file=known_hosts=gocd_known_hosts\n\n# Chart version 1.37.0 is gocd v21.2.0\nsudo helm install -f gocd_values.yaml gocd gocd/gocd --version 1.37.0\nsudo chcon -R -t container_file_t /kube/pvc\n# Installs a trash service on port 80 by default. Let's delete it:\nsudo kubectl delete ingress gocd-server\n# Instead of using \"kubectl scale\", scale agents by editing gocd_values.yaml\n# and do \"sudo helm upgrade -f ....\"\nsudo helm install -f gocd_values.yaml gocd gocd/gocd --version 1.37.0\n# Make sure to enable the agents in the web UI, and assign them to Resources and Environments.\n```\n\n#### Upgrade process (**make sure to `sudo kubectl delete ingress gocd-server` after every upgrade**):\n\n```\n# Disable and Delete all agents in the AGENTS tab of gocd.\nEdit gocd_values.yaml and set agent version to latest (e.g. v21.2.0-groups-0.5.8)\nsudo helm upgrade -f gocd_values.yaml gocd gocd/gocd --version 1.37.0\nsudo kubectl delete ingress gocd-server\n# Upgrade gocd-agent-dind:\nsudo kubectl patch deployments/gocd-agent-dind --patch '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"gocd-agent-dind\",\"image\":\"gocd/gocd-agent-docker-dind:v21.2.0\"}]}}}}'\n# You can also use patch to upgrade gocd-agent and gocd-server instead of upgrading via helm chart. I sometimes do it that way.\n# Delete the old Agents on the web interface. Wait for the new agents to come up, and enable them and assign them as appropriate.\n```\n\n#### DockerHub\n\nCreate DockerHub permissions: Create an account if you do not have one. Visit https://hub.docker.com/settings/security and create an Access Token. Copy the token.\n\n```bash\nsudo  kubectl create secret docker-registry regcred --docker-server=https://index.docker.io/v1/ --docker-username=dockerhubuser --docker-password=XXXX --docker-email=your.dockerhub.email@example.com\n```\n\n#### OSXCROSS\n\nFor osxcross (OSX cross compiling), make sure the OSX compiler SDK *you created on mac* os located at /opt/osxcross.\n\nIf you do not have osxcross (no access to Apple hardware), then you must create an empty directory at /opt/osxcross or remove the relevant volume mounts (and remove from enabled build targets in the gocd pipelines).\n\nNow create Docker-in-Docker, and apply this file with `kubectl create -f dind.yaml` or `kubectl apply -f dind.yaml`\n\n#### dind.yaml\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: gocd\n    component: agent-dind\n  name: gocd-agent-dind\n  namespace: default\nspec:\n  progressDeadlineSeconds: 600\n  replicas: 2\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: gocd\n      component: agent-dind\n      release: gocd\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 25%\n    type: RollingUpdate\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: gocd\n        component: agent-dind\n        release: gocd\n    spec:\n      containers:\n      - env:\n        - name: GO_SERVER_URL\n          value: http://gocd-server:8153/go\n        image: gocd/gocd-agent-docker-dind:v21.2.0\n        imagePullPolicy: IfNotPresent\n        name: gocd-agent-dind\n        resources: {}\n        securityContext:\n          privileged: true\n        terminationMessagePath: /dev/termination-log\n        terminationMessagePolicy: File\n        volumeMounts:\n        - mountPath: /root/.ssh\n          name: ssh-secrets\n          readOnly: true\n        - mountPath: /root/.docker\n          name: kaniko-secret\n        - mountPath: /opt/osxcross\n          name: opt-osxcross-volume\n          readOnly: true\n      dnsPolicy: ClusterFirst\n      restartPolicy: Always\n      schedulerName: default-scheduler\n      securityContext:\n        fsGroup: 0\n        runAsGroup: 0\n        runAsUser: 0\n      serviceAccount: default\n      serviceAccountName: default\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: ssh-secrets\n        secret:\n          defaultMode: 256\n          secretName: gocd-ssh\n      - name: kaniko-secret\n        secret:\n          secretName: regcred\n          items:\n            - key: .dockerconfigjson\n              path: config.json\n      - name: opt-osxcross-volume\n        hostPath:\n          path: /opt/osxcross\n          type: Directory\n```\n\n### OSXCROSS GOCD Agents\n\nTo add osxcross support to the main agents, create the following patch:\n\ngocd-agent-patch.yaml\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gocd-agent\nspec:\n  template:\n    spec:\n      containers:\n      - name: gocd-agent\n        volumeMounts:\n        - mountPath: /opt/osxcross\n          name: opt-osxcross-volume\n          readOnly: true\n      volumes:\n      - name: opt-osxcross-volume\n        hostPath:\n          path: /opt/osxcross\n          type: Directory\n```\n\nand apply with:\n```bash\nsudo kubectl patch deployments/gocd-agent -p \"$(cat gocd-agent-patch.yaml)\"\n```\n\nMake sure to enable the Agents when they come up on the GoCD Dashboard. Add every server to the \"`developement`\" environment. Also, assign linux servers to \"`mingw`\" and \"`linux`\". Assign the dind agents to \"`dind`\".\n\n\nFor GitLab, go to https://ci.v-sekai.cloud/go/admin/security/auth_configs and select `Create new authorization configuration` -> `gitlab-auth-config` / `GitLab Authentication plugin` / follow documentation here: https://github.com/gocd-contrib/gitlab-oauth-authorization-plugin/blob/master/INSTALL.md - **Do not check Allow only known users to login yet**. If this works, you can skip the text auth step and corresponding passwd commands.\n\nCreate Guest login:\n1. Go to auth_configs, `Create new authorization configuration` -> `guest-login` / `Guest Login Plugin` / Fill out Go server URL / Username `view` / Display name `Guest`.\n2. Now, go to Roles Management. Create role `guest`. Add Deny for all types and Resources `*` as desired.\n3. In an Incognito window, visit the CI system and login as Guest. Close the incognito window.\n4. Now, go to Users Management. Select view / Guest and select Roles -> `guest`\n5. Admin -> Pipelines. Select Pipeline Group `beta`, click + on the top right of the whole *group*, go to Roles, add `guest`, and only check **View**. Save this.\n\nAt this point, Guest should have permission to view pipelines, see logs, download artifacts but nothing else.\n\nFor text auth, go to https://ci.v-sekai.cloud/go/admin/security/auth_configs and select `Create new authorization configuration` -> `file-auth-config` / `Password File Authentication plugin` / `/godata/config/password.properties`\n\n```bash\nsudo kubectl exec gocd-server-6d77846995-5l244 -- touch /godata/config/password.properties\nsudo yum install httpd-tools\nhtpasswd -c -B passwd admin\ncat passwd | sudo kubectl exec gocd-server-6d77846995-5l244 -- sudo tee /kubepvc/pvc/*/godata/config/password.properties\n```\n\nNow go to users page, edit your user and enable `Global Admin`.\n\nNow go to file-auth-config, edit configuration, enable Allow only known users to login\n\n### gocd config repositories:\n\nGo to *ADMIN -> Config Repositories*\n- **Config repository Name:** groups-gocd-pipelines\n- **Plugin ID:** JSON Configuration Plugin\n- **Material Type:** Git\n- **URL:** git@gitlab.com:godot-groups/groups-gocd-pipelines\n- **Branch:** master\n- **GoCD pipeline files pattern:** *.gopipeline.json\n- **GoCD environment files pattern:** *.goenvironment.json\nRULES\n- **Allow:** Pipeline Group: beta\n- **Allow:** Environment: development\n\n### Setup of flux GitOps:\n\n```bash\nwget https://github.com/fluxcd/flux/releases/download/1.20.2/fluxctl_linux_amd64\nsudo cp fluxctl_linux_amd64 /usr/local/bin/fluxctl\nsudo chmod +x /usr/local/bin/fluxctl\nsudo helm repo add fluxcd https://charts.fluxcd.io\nsudo kubectl apply -f https://raw.githubusercontent.com/fluxcd/helm-operator/master/deploy/crds.yaml\nsudo kubectl create namespace flux\nsudo kubectl identity --k8s-fwd-ns flux\n```\n\nFork the flux-config repository from here https://github.com/V-Sekai/flux-config into your own github account, and set GHUSER=your github account.\n\nNow, in your fork of flux-config, go to project Settings -> Deploy Keys and add the result of the above identity command. Make sure to check **Allow write access**.\n\nOnce you have done this, you can continue with the flux setup using your newly forked repository.\n\n```bash\nexport GHUSER=\"xxxxxxxxx\"\nsudo fluxctl install --git-user=${GHUSER} --git-email=${GHUSER}@users.noreply.github.com - -git-url=git@github.com:${GHUSER}/flux-config --git-path=workloads --namespace=flux > fluxcmd_install.yaml\nsudo kubectl apply -f fluxcmd_install.yaml\nsudo fluxctl --k8s-fwd-ns flux\nsudo fluxctl list-workloads --k8s-fwd-ns flux\n```\n\n\nFOR DEBUGGING ONLY: `sudo setenforce permissive` - this appears to have no effect, so there is a different problem.\n\n### Setting up cockroachdb:\n\n#### cockroachdb.values.yaml\n\n```yaml\nstatefulset:\n  resources:\n    limits:\n      memory: \"8Gi\"\n    requests:\n      memory: \"8Gi\"\nconf:\n  cache: \"2Gi\"\n  max-sql-memory: \"2Gi\"\ntls:\n  enabled: true\n```\n\n#### CockroachDB install\n\n```bash\nsudo helm install cockroachdb --values cockroachdb.values.yaml cockroachdb/cockroachdb\nsudo kubectl certificate approve default.node.cockroachdb-0\nsudo kubectl certificate approve default.node.cockroachdb-1\nsudo kubectl certificate approve default.node.cockroachdb-2\nsudo kubectl certificate approve default.client.root\n\ncurl -o client-secure.yaml https://raw.githubusercontent.com/cockroachdb/cockroach/master/cloud/kubernetes/client-secure.yaml\nsudo kubectl apply -f client-secure.yaml\nsudo kubectl exec -it cockroachdb-client-secure -- ./cockroach sql --certs-dir=/cockroach-certs --host=cockroachdb-public\n```\n\nIn SQL, write:\n\n```sql\nCREATE DATABASE uro_prod;\nCREATE USER 'uro-prod' WITH PASSWORD 'blablablablaSOMEDATABASEPASSWORD';\nGRANT ALL ON DATABASE uro_prod to \"uro-prod\";\n```\n\nTo make backups:\n\n```bash\nsudo kubectl exec -it cockroachdb-client-secure -- ./cockroach dump --certs-dir=/cockroach-certs --host=cockroachdb-public uro_prod > uro_prod_backup.txt\n```\n\nApply secrets:\n\n```bash\n(On dev machine) MIX_ENV=prod mix phx.gen.secret\n# Copy the output of above, and copy the database password from above:\nsudo kubectl create secret generic uro-prod --from-literal=secret-key-base='GENERATED_WITH_ABOVE_COMMAND' --from-literal=pass='blablablablaSOMEDATABASEPASSWORD'\n\nsudo kubectl apply -f https://raw.githubusercontent.com/V-Sekai/uro/master/kubernetes.yaml\n```\n\n### Keeping the system and cluster up-to-date\n\n#### Upgrading fedora\n\nStart the upgrade process with:\n\n```bash\nsudo dnf upgrade --refresh\nsudo dnf install dnf-plugin-system-upgrade\nsudo dnf system-upgrade download --releasever=34\n```\n\nAnswer `y` to all prompts confirming list of packages and new GPG keys, if any. Once successful, it displays:\n\n```\nDownload complete! Use 'dnf system-upgrade reboot' to start the upgrade.\nTo remove cached metadata and transaction use 'dnf system-upgrade clean'\nThe downloaded packages were saved in cache until the next successful transaction.\nYou can remove cached packages by executing 'dnf clean packages'.\n```\n\nComplete the upgrade with:\n\n```bash\nsudo dnf system-upgrade reboot\n```\n\nThis will bring the system down for about an hour.\n\n#### Upgrading nginx and cert-manager\n\n```bash\nsudo helm repo update\n# If the stable/nginx-ingress chart is still installed, make sure to helm uninstall nginx first.\nsudo helm upgrade --namespace ingress-nginx nginx ingress-nginx/ingress-nginx --set controller.replicaCount=2\nsudo helm upgrade cert-manager jetstack/cert-manager --namespace cert-manager --set installCRDs=true\n### I was not able to get external-dns to upgrade, but it is not user-facing so we keep running the old version.\n# sudo helm upgrade external-dns --reuse-values bitnami/external-dns\n```\n\n#### Upgrading k3s\n\n```bash\ncurl -sfL https://raw.githubusercontent.com/rancher/k3s/master/install.sh | sh -s - server -o /root/.kube/config --default-local-storage-path /kube/pvc --no-deploy=servicelb --disable=traefik --disable=servicelb\n```\n\n#### Upgrading cockroachdb\n\nRun the shell:\n\n```bash\nsudo kubectl exec -it cockroachdb-client-secure -- ./cockroach sql --certs-dir=/cockroach-certs --host=cockroachdb-public\n```\n\nAt the top of the shell prompt, it will say something like this. Copy the first two numbers from the CockroachDB CCL line. In this case, `20.2`\n\n```bash\n#\n# Welcome to the CockroachDB SQL shell.\n# All statements must be terminated by a semicolon.\n# To exit, type: \\q.\n#\n# Server version: CockroachDB CCL v20.2.2 (x86_64-unknown-linux-gnu\n```\n\nReplace `some.version` in the following command with the value we found above:\n\n```sql\nSET CLUSTER SETTING cluster.preserve_downgrade_option = 'some.version';\n```\n\nFinally, perform the upgrade:\n\n```bash\nsudo kubectl delete job cockroachdb-init\nsudo helm upgrade cockroachdb cockroachdb/cockroachdb --values cockroachdb.values.yaml\ncurl -o client-secure.yaml https://raw.githubusercontent.com/cockroachdb/cockroach/master/cloud/kubernetes/client-secure.yaml\nsudo kubectl delete pods/cockroachdb-client-secure\nsudo kubectl create -f client-secure.yaml\n```\n\n## Positive Consequences <!-- optional -->\n\n- Knowledge gains about service creation\n- Operation experience\n\n## Negative Consequences <!-- optional -->\n\n- Constant maintenance\n\n## Option graveyard: <!-- same as above -->\n\n* Option: Dedicated Servers\n* Option: Virtual Machines\n\n## If this enhancement will not be used often, can it be worked around with a few lines of script?\n\nYes, it is possible to deploy this system using docker or in the root system.\n\n## Is there a reason why this should be core and done by us?\n\nHaving experience with kubernetes and maintaining discipline will make scaling or service upgrades smoother in the future.\n\n## Links <!-- optional -->\n\n- https://k3s.io/\n"},"creationDate":"2021-05-29T18:26:18.000Z","lastEditDate":"2021-05-29T18:49:30.000Z","lastEditAuthor":"K. S. Ernest (iFire) Lee","publicationDate":null,"file":{"relativePath":"docs/decisions/20210529-manage-backend-resources-on-one-or-more-physical-machines-with-k3s.md","absolutePath":"/home/runner/work/v-sekai-proposals/v-sekai-proposals/docs/decisions/20210529-manage-backend-resources-on-one-or-more-physical-machines-with-k3s.md"},"repository":{"provider":"github","viewUrl":"https://github.com/V-Sekai/v-sekai-proposals/blob/master/docs/decisions/20210529-manage-backend-resources-on-one-or-more-physical-machines-with-k3s.md"}},"l4bVersion":"1.0.0-beta.11"},"__N_SSG":true}