# Proposed: Triangle Meshes Inferencing with MeshRWKV

## Metadata

- Status: Proposed
- Deciders: V-Sekai
- Tags: V-Sekai

## Background

MeshGPT uses a transformer model trained on geometric vocabulary to generate triangle meshes, resulting in clean, coherent, and compact meshes.

## Problem

The challenge is adapting GPT-like generative models for the 3D domain. Automated triangle mesh generation needs a robust solution.

## Strategy

The MeshRWKV model implementation is a sequence-based approach that autoregressively generates triangle meshes as sequences of triangles. The conversion process to RWKV4 can be divided into two main steps:

### Step 1: Learning a Vocabulary for Triangle Meshes

The first step involves learning a vocabulary capable of accurately representing triangle meshes from a large collection of 3D object meshes. This allows triangles to be encoded and decoded from this embedding.

A graph convolution encoder is used, which operates on the triangles of a mesh and their neighborhood to extract geometrically rich features that capture the intricate details of 3D shapes. These features are quantized as embeddings of a codebook using residual quantization, effectively reducing the sequence lengths of the mesh representation.

These embeddings are sequenced and then decoded by a 1D ResNet guided by a reconstruction loss. This phase lays the groundwork for the subsequent training of the transformer.

### Step 2: Autoregressive Mesh Generation

The next step is to train RWKV4, which leverages these quantized geometric embeddings. Given a sequence of geometric embeddings extracted from the triangles of a mesh, the transformer is trained to predict the codebook index of the next embedding in the sequence.

Once trained, the transformer can be auto-regressively sampled to predict sequences of embeddings. These embeddings can then be decoded to generate novel and diverse mesh structures that display efficient, irregular triangulations similar to human-crafted meshes.

### Ablations

In Table 3, a set of ablations on the task of unconditional mesh generation on ShapeNet Chair category is shown. Further ablations are detailed in the supplementary.

- Naive tokenization(w/o Learned Tokens) and naive per face quantization (w/o per-vertex quantization) markedly diminishes shape quality.
- Longer sequences without sequence compression (w/o Sequence Compression) lead to the model forgetting the context and repeating shape elements in the output.
- Using geometric embeddings in vocabulary learning significantly improves over naive coordinate tokenization (w/o Learned Tokens).
- A model with a shorter sequence length performs better than without (w/o Sequence Compression), as shorter sequence lengths fit transformer context windows better.
- An alternative to having embeddings aggregated and quantized across vertex indices, is to simply have the same number of embeddings directly per face (w/o per Vertex Quantization). This makes sequences much harder to learn with the transformer.
- An alternative to using embeddings from the graph encoder and the codebook is to only use the codebook indices of these tokens as input to the transformer, and let the transformer learn the discrete token embeddings (w/o Encoder Features). While the transformer is able to still learn meaningful embeddings, these are still not as effective as using graph encoder features.
- Training only on shapes from individual categories (w/o Pretraining) leads to overfitting and suboptimal performance, in contrast to pre-training our GPT transformer on all ShapeNet train shapes.

#### Limitations

MeshGPT significantly advances direct mesh generation but faces several limitations. Its autoregressive nature leads to slower sampling performance, with mesh generation times taking 30 to 90 seconds. Despite our learned tokenization approach reducing sequence lengths, which suffices for single object generation, it may not be as effective for scene-scale generation, suggesting an area for future enhancement. Moreover, our current computational resources limit us to using a GPT2-medium transformer, which is smaller than more sophisticated models like Llama2 [58]. Given that larger language models benefit from increased data and computational power, expanding these resources could significantly boost MeshGPT’s performance and capabilities.

### Key Terms

- **Encoder-Decoder Network:** A type of artificial neural network designed to convert inputs into a set of features, and then decode those features to produce a new output.
- **Residual Quantization:** A technique used to reduce the sequence lengths of the mesh representation.
- **1D ResNet:** A type of convolutional neural network that is used to decode the sequenced embeddings.
- **RWKV4:** A robust and efficient model architecture called Receptance Weighted Key Value (RWKV) that we're using to train our MeshRWKV model for Elixir.
- **MeshRWKV:** The model we're proposing. It's specifically trained for Elixir using RWKV4 and uses an encoder-decoder network with vector quantization to generate triangle meshes from a learned vocabulary.
- **Autoregressive Model:** A type of statistical model used for time series data. In our case, we're using it to generate a mesh based on the sequence of words from our learned vocabulary.
- **Elixir:** A functional, concurrent, general-purpose programming language that runs on the Erlang virtual machine (BEAM). We're training our MeshRWKV model specifically for this language.
- **Triangle Meshes:** A type of polygon mesh commonly used in 3D computer graphics. They're composed entirely of triangular facets. Our model is designed to generate these meshes from a learned vocabulary.
- **Geometric Embeddings:** These are representations of triangles in a high-dimensional space that capture their geometric properties.
- **Graph Convolution Encoder:** This is a type of neural network architecture designed to operate on graph-structured data.
- **Auto-regressive Sampling:** A method of generating new data points based on previously generated ones.

## Benefits

Our approach produces compact meshes with sharp geometric details. It can suggest multiple shape completions for incomplete shapes.

## Drawbacks

Training the model may require significant computational resources. Learning a vocabulary for triangle meshes could be complex.

## Alternatives

Other methods like neural field-based approaches exist but often miss geometric details or produce over-triangulated meshes.

## Special Use Case

Generating complex 3D shapes for niche applications might need additional model training or fine-tuning.

## Core & Implemented by Us?

Yes, this proposal is core to our work at V-Sekai and will be implemented by us.

## References

1. [V-Sekai · GitHub](https://github.com/v-sekai)
2. [V-Sekai/v-sekai-game](https://github.com/v-sekai/v-sekai-game)
3. [MeshGPT](https://nihalsid.github.io/mesh-gpt/)

Assisted by AI assistant Aria.

## MeshGPT's Model and Metrics

In this experiment, a computer model is trained to generate 3D shapes. The training uses a dataset known as ShapeNetV2, which includes various categories of 3D shapes. The model is specifically fine-tuned, meaning it receives special training on four categories: Chair, Table, Bench, and Lamp.

During the training phase, several techniques are employed to enhance the model's ability to create diverse shapes. These techniques include random shifts (altering parts of the shape), scaling (modifying the size of the shape), and planar decimation (simplifying the shape).

The performance of the model is evaluated using several metrics:

- Minimum Matching Distance (MMD): This metric measures how closely the generated shape matches the real one. A lower MMD value indicates a closer match.
- Coverage (COV): This metric assesses how much of the actual shape is covered by the generated shape. A higher COV value is preferable.
- 1-Nearest-Neighbor Accuracy (1-NNA): This metric evaluates how frequently the closest shape to the generated one is the actual one. A score of 50% is considered optimal.
- Chamfer Distance (CD): This is a method for measuring distance in 3D.

To evaluate the visual similarity between the generated shapes and the actual ones, both are rendered as images from eight different viewpoints using a software called Blender. Two scores, FID and KID, are then calculated to measure the similarity between these image sets. Lower FID and KID scores indicate greater similarity.

Lastly, compactness is measured, which is the average number of vertices (corners) and faces (flat surfaces) in the generated shapes.

The results demonstrate that the MeshGPT model outperforms other models in generating high-quality, visually similar, and compact 3D shapes.
